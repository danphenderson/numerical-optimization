\documentclass[10pt]{article}

\input{../preamble.tex}

% ----------------- Document TITLE
% -----------------------------------------------------------------
% ------------------
\title{Classification of Smooth Unconstrained Optimization Problems}
\author{Daniel Henderson}
\date{\today}

% ------------------ DOCUMENT START
% -----------------------------------------------------------------
% ------------------
\begin{document}
\maketitle

% ------------------ ABSTRACT
% -----------------------------------------------------------------
% ------------------
\begin{abstract}
    \noindent
    \textcolor{Red}{
    We study the local convexity properties of a benchmark suite of smooth,
    unconstrained minimization problems drawn from the
    \texttt{OptimizationProblems.jl}~\cite{OptimizationProblems} Julia package.
    For each problem, we review its origin, present the analytic form
    of the objective $f:\R^n\!\to\!\R$ and the standard starting point
    $\vct{x}_0$.  We introduce a sampling-based procedure to classify
    the critical-point structure and verify the positive-definiteness of
    the Hessian in a neighborhood of a strict local minimizer.  Numerical
    experiments confirm that while some test functions exhibit strong
    local convexity, others contain narrow regions of non-convexity that
    can slow down standard schemes.  Our findings provide guidance for
    choosing and tuning first- and second-order methods on common
    benchmark problems.
    \textbf{TODO: Update this with the final results.}
    }
    \noindent\textbf{Keywords:} Julia, Optimization, Benchmarking, Automatic Differentiation
\end{abstract}



% ------------------ TABLE OF CONTENTS
% -----------------------------------------------------------------
% ------------------
\tableofcontents
\newpage


% ------------------ SECTION: INTRODUCTION
% -----------------------------------------------------------------
% ------------------
\section{Introduction}
    \label{sec:intro}
    \medskip
    Understanding the local convexity of an objective function
    near strict local minimizers is fundamental to the design 
    and convergence analysis of smooth optimization algorithms. 
    Strong local convexity near a minimizer often guarantees rapid convergence 
    of gradient-based and Newton-type methods, whereas narrow or nonconvex regions 
    can result in slow progress or convergence to saddle points. 
    Standard benchmark suites such as the Constrained and Unconstrained Testing Environment 
    (CUTE)~\cite{CUTE} offer a variety of smooth test functions with diverse convexity profiles, 
    but a systematic comparison of their local convexity properties is lacking.
    In this work, we analyze the local convexity of a smooth objective functions
    within neighborhoods of strict local minimizers. We focus on a subset of objective functions
    from the \emph{Constrained and Unconstrained Testing Environment} (CUTE)~\cite{CUTE}.\

    \medskip

    Our contributions are:
    \textcolor{Blue}{
        \begin{itemize}
            \item We review existing techniques for classifying critical points.
            \item We present a sampling-based classification algorithm for determining the local convexity
            characteristics of the objective function $f$ in a neighborhood of a strict local minimizer.
            \item  \textbf{TODO:} Add other contributions
        \end{itemize}
    }\
    
    \medskip

    Our work is organized as follows.
    \begin{itemize}
        \item Section~\ref{sec:theory} presents a theoretical framework for classifying critical points
        using dynamical systems, Morse theory and spectral analysis of $\nabla^2 f(\vct{x}^*)$.
        \item Section~\ref{sec:problems} describes our problem selection criteria and 
        provide analytical expressions for the objective function $f$ and the
        accepted $\vct{x}_0$ for each problem. We include a brief discussion of
        their provenance and known properties from the literature. 
        \item Section~\ref{sec:procedures} discuss our numerical experiments for determining
        strict local minimizers $\vct{x}^*$ and details our sampling-based classification algorithm
        for determining the local convexity characteristics of $f$ in a neighborhood of $\vct{x}^*$.
        \item Section~\ref{sec:conclusion} summarizes our findings and concludes with practical 
        recommendations and mentions future work. Concludes with a discussion of the implications of our findings,
        and how they can be used to inform the design and tuning of optimization algorithms.
    \end{itemize}

    Our introduction proceeds as follows. We fix the notation used throughout this 
    paper in ~\ref{intro:preliminaries}. The general problem is stated in \ref{intro:unc-problem}
    and we discuss a \emph{continuous model}~\ref{intro:grad-flow} for solving the smooth minimization
    problem. We rigoursly define a \emph{scheme}, discussing the properties of first and second order
    schemes in detail. We conclude the introduction by listing the benchmark problems 
    under study~\ref{intro:problems} and describing their origins in the literature.

    % Intro - Preliminaries
    \subsection{Preliminaries}
        \label{intro:preliminaries}
        \medskip 

        We use the notation defined in the Appendix~\ref{notation} as used by 
        Nocedal and Wright~\cite{NocedalAndWright06}.
        
        Unless stated otherwise we assume $f: \R^n \rightarrow \R$ is twice 
        continuously differentiable. Throughout we restrict our attention to
        a bounded path-connected open subset $\Omega \subset \R^n$ chosen large 
        enough to contain all iterates and flow trajectories considered in this 
        study. We choose our domain of interest $\Omega$ so the boundary 
        $\partial\Omega : = \overline{\Omega} \setminus \Omega$ is sufficiently
        smooth, meaning that $f$ and its derivatives extend continuously to the boundary.

        \todo{Add assumptions boxes used in paper as neccesasary}

    % Intro - Unconstrained Optimization Problem
    \subsection{Unconstrained Optimization Problem}
        \medskip
        \label{intro:unc-problem}
        Consider the general form of a \textbf{smooth unconstrained optimization problem}
        \begin{equation}
            \min_{\vct{x} \in \mathbb{R}^n} f(\vct{x}), \label{eq:unconstrained-opt}
        \end{equation}
        where $\vct{x} \in \R^n$ is the optimization variable and $f: \R^n \rightarrow \R$ is
        a twice continuously differentiable objective function.    

        If a solution of \eqref{eq:unconstrained-opt} exists, then we denote the 
        \textbf{optimal value} and \textbf{optimal solution} as
        \begin{flalign*}
            \vct{x}^{*}\in\arg\min_{\vct{x}}f(\vct{x})
            \quad \text{ and } \quad
            f^* = \mathop{\mathrm{min}}_{\vct{x} \in \R^n} f(\vct{x}),
        \end{flalign*}

        In practice we choose $\Omega$ to be the problem domain of interest.
        By the standing assumptions of \ref{intro:preliminaries},
        $\Omega$ is a bounded, path-connected, open subset of $\R^n$ so
        the closure $\overline{\Omega}$ is a bounded and closed subset of
        $\R^n$. The Heine-Borel theorem implies that the closure $\overline{\Omega}$ is compact.
        Consequently, the extreme value theorem asserts that a continuous real-valued function $f$
        obtains a maximum and minimum value on the compact set $\overline{\Omega}$, where continuity
        of $f$ on $\overline\Omega$ is guaranteed by the standing $C^{2}$ assumption.
        So by us choosing a problem domain of interest, we can guarantee an optimal solution
        exists in $\overline{\Omega}$ and we can make global optimality claims about the optimal
        value within our chosen domain. 

        \medskip
        
        A point satisfying the first-order optimality condition is called a \emph{critical point}. 
        Such a point can be a local minimizer, local maximizer, or saddle point. We classify critical 
        points as follows (cf.~\cite{NocedalAndWright06}):
        \begin{itemize}
            \item A \emph{local minimizer} if there exists a neighborhood $N$ of $\vct{x}^*$ such that 
            $f(\vct{x}^*) \leq f(\vct{x})$ for all $\vct{x} \in N$.
            \item A \emph{strict local minimizer} if there exists a neighborhood $N$ of $\vct{x}^*$ 
            such that $f(\vct{x}^*) < f(\vct{x})$ for all $\vct{x} \in N \setminus \{\vct{x}^*\}$.
            \item An \emph{isolated local minimizer} if there exists a neighborhood $N$ of 
            $\vct{x}^*$ such that $\vct{x}^*$ is the unique local minimizer in $N$.
            \item A \emph{global minimizer} if $f(\vct{x}^*) \leq f(\vct{x})$ for all $\vct{x} \in \R^n$.
        \end{itemize}

        Since we focus on nonlinear objective functions, global optimality generally cannot be 
        guaranteed. However, local optimality of a critical point $\vct{x}^*$ is verified using 
        the following conditions:
        \begin{flalign*}
            \text{Necessary:} \quad  & \nabla f(\vct{x}^*) = 0 ~ \text{ and } ~ \nabla^2 f(\vct{x}^*) \succeq 0\\
            \text{Sufficient:} \quad & \nabla^2 f(\vct{x}^*) \succ 0. 
        \end{flalign*}
        The necessary condition for $\vct{x}^*$ to be a local minimizer of $f$ is that 
        $\vct{x}^*$ satisfies the first-order and second-order optimality conditions.
        The \emph{second-order} optimality condition states that the Hessian of $f$ 
        is positive semidefinite at $\vct{x}^*$. If the Hessian is strictly positive
        definite at $\vct{x}^*$, it is both a necessary and a sufficient condition that $\vct{x}^*$
        is a strict local minimizer. It holds that strict local minimizers are isolated,
        corresponding precisely to points where $f$ is strictly convex.\\
    
    % Intro - Continuous Model
    \subsection{Continuous Model}
        \label{intro:grad-flow}
        \medskip

        We reinterpret the minimization problem~\eqref{eq:unconstrained-opt} as the search for equilibria 
        of the \emph{gradient flow} dynamical system. Interpreting our problem as a dynamical system 
        allows us to:

        \begin{itemize}
            \item exploit geometric structure when analyzing the critical-point landscapes;
            \item design time-discretisations of the gradient flow ODE~\eqref{eq:gradient-flow}
            and construct error bounds using the continuous model;
            \item leverage the stability theory of autonomous ODEs on compact manifolds.
        \end{itemize}

        In this section, we establish a link between the minimization of $f$ and the 
        trajectories of a dynamical system.

        \begin{definition}
            \label{def:grad-flow}
            The \emph{gradient‐flow} dynamical system is defined by the IVP 
            \begin{equation}
                \label{eq:gradient-flow}\tag{GF}
                \vct{\gamma}'(t) = -\nabla f\bigl(\vct{\gamma}(t)\bigr) ~\text{ subject to } \vct{\gamma}(0) = \vct{x}_0 \in \overline\Omega
            \end{equation}
            An integral curve $\vct{\gamma}$ satisfying the \emph{gradient-flow} IVP is a gradient \emph{flow-line}, or, \emph{trajectory}.
        \end{definition}

        Suppose the initial point $\vct{x_0}$ is a critical point, then the gradient flow ODE 
        is satisfied by the constant trajectory $\vct{\gamma}(t) = \vct{x}_0$ for any time $t$. 
        Notice that constant $\vct{\gamma}(t)$ implies $\vct{\gamma}'(t) = \vct{0}$, substituting
        into the gradient-flow ODE asserts that $-\nabla f(\vct{x}_0) = \vct{0}$, which holds true
        since $\vct{x_0}$ is a critical point (i.e., $\nabla f(\vct{x}_0) = \vct{0}$).
        Consequently, the existence of a solution holds when $\vct{x}_0$ is a critical point
        and such points correspond to stationary equilibria in the gradient flow ODE phase space.
        Now we show equation \eqref{eq:gradient-flow} is well-posed for all $\vct{x}_0 \in \overline{\Omega}$.

        Let $\vct{\gamma}_{\vct{x_0}}$ be any gradient \emph{flow-line} starting at
        some point $\vct{x_0}$ in $\overline{\Omega}$. Note that the trajectory $\vct{\gamma}_{\vct{x_0}}$ 
        is driven by the steepest descent direction of the objective function $f$. But $f$ is bounded
        below by the minimum value $f^*$, so the trajectory $\vct{\gamma}_{\vct{x_0}}$ cannot
        escape the compact set $\overline{\Omega}$ in finite time. Indeed, we will show that the trajectory 
        $\vct{\gamma}_{\vct{x_0}}$ is guaranteed to converge to a critical point of $f$ in $\overline{\Omega}$.
        But first we must establish the IVP~\eqref{eq:gradient-flow} is well-posed for all $\vct{x}_0 \in \overline{\Omega}$.

        \begin{theorem}[Existence]\label{thm:gf-existence}
            For every $\vct{x_0}\in \overline{\Omega}$ the IVP \eqref{eq:gradient-flow} admits a solution
            $\gamma_{\vct{x_0}}\in C^{1}\bigl([0,\infty);\overline{\Omega}\bigr)$
            such that $\vct{\gamma}_{\vct{x_0}}(t) \in \overline{\Omega}$ for all $t\ge 0$. 
        \end{theorem}

        \begin{proof}[Sketch]
            The gradient flow ODE \eqref{eq:gradient-flow} is a first-order, autonomous,
            ordinary differential equation (ODE) on the compact manifold $\overline{\Omega}$.
            To apply the Picard-Lindelöf theorem, it is sufficient to note $f \in C^{2}(\overline{\Omega})$
            implies that $-\nabla f \in C^{1}(\overline{\Omega})$, and it follows that $\nabla f$ is
            Lipschitz on $\overline{\Omega}$, (i.e., $f$ is $\ell$-smooth). Consequently, the gradient 
            flow ODE \eqref{eq:gradient-flow} is well-posed for all interior initial conditions 
            $\vct{x}_0 \in \Omega$ for some finite time $t^*$.\
            \td{
                TODO: 
                - Consider cleanest way to introduce the geometric approach without getting to deep into the
                topological / differential geometry of manifolds.
                - The simplest approach to "well-posedness" seems to be restricting our initial condition to
                the $\Omega$, as we assume for the input of a scheme, then show that the flow lines are contained in
                the closure of the manifold $\overline{\Omega}$ by invoking the monotonicity of $f$ along the flow lines
                and the fact that the image of $f(\overline{\Omega})$ is a compact convex (and path-connected?) set.
                - Define level sets, sub level sets, and the signed distance function, then introduce the alternative 
                continuous model describing the flow of geodesics. Incorporate aspects of the Morse Theory section.
            }
        \end{proof}

        The following lemma characterizes the monotonicity of $f$ along the trajectory $\vct{\gamma}_{\vct{x_0}}$.

        \begin{lemma}\label{lem:monotone-flow-lines}
            Along any trajectory $\vct{\gamma}_{x_0}(t)$ one has
            \begin{equation}\label{eq:monotone-flow-lines}
                \frac{d}{dt}f\bigl(\vct{\gamma}_{\vct{x_0}}(t)\bigr) = -\|\nabla f\bigl(\vct{\gamma}_{\vct{x_0}}(t)\bigr)\|^{2}\;\le\;0, \quad t \ge 0.
            \end{equation}
            Hence $f\circ\vct{\gamma}_{x_0}$ is non‐increasing in $t$, and strictly decreasing whenever $\nabla f(\vct{\gamma}_{\vct{x_0}}(t))\neq0$.
        \end{lemma}

        \begin{proof}
            Differentiating the composition $f\circ\vct{\gamma}_{\vct{x_0}}$ w.r.t. $t$ using 
            the chain rule yields
            \begin{flalign*}
                \frac{d}{dt}f\bigl(\vct{\gamma}_{\vct{x_0}}(t)\bigr) & = \nabla f\bigl(\vct{\gamma}_{\vct{x_0}}(t)\bigr) \cdot 
                    \frac{d}{dt}\vct{\gamma}_{\vct{x_0}}(t)\\
                & = \nabla f\bigl(\vct{\gamma}_{\vct{x_0}}(t)\bigr) \cdot \vct{\gamma}'_{\vct{x_0}}(t) \\
                & = \nabla f\bigl(\vct{\gamma}_{\vct{x_0}}(t)\bigr) \cdot \left(-\nabla f\bigl(\vct{\gamma}_{\vct{x_0}}(t)\bigr)\right)\\
                & = \langle \nabla f\bigl(\vct{\gamma}_{\vct{x_0}}(t)\bigr), -\nabla f\bigl(\vct{\gamma}_{\vct{x_0}}(t)\bigr) \rangle\\
                & = -\langle \nabla f\bigl(\vct{\gamma}_{\vct{x_0}}(t)\bigr), \nabla f\bigl(\vct{\gamma}_{\vct{x_0}}(t)\bigr) \rangle\\
                & = -\|\nabla f\bigl(\vct{\gamma}_{\vct{x_0}}(t)\bigr)\|^{2} \\
                & \leq 0.
            \end{flalign*}
            A direct consequence of the positive-definiteness property of a norm is that the final inequality
            is strict for all points $\vct{\gamma}_{\vct{x_0}}(t)$ that aren't critical.
        \end{proof}

        \begin{theorem}
            For all $\vct{x}_0 \in \overline{\Omega}$, the flow operator maps to a unique integral 
            curve $\vct{\gamma}_{\vct{x}_0}(t)$ of the gradient flow ODE \eqref{eq:gradient-flow} such that
            \begin{equation}\label{eq:flow-map-curve}
                \vct{\gamma}_{\vct{x}_0}(t) = T(\vct{x}_0; t) : = \vct{x}_0 - \int_0^t \nabla f\bigl(\vct{\gamma}_{\vct{x}_0}(s)\bigr) ds
            \end{equation}
            and $\vct{\gamma}_{\vct{x}_0}(t)$ is a continuous map from $\R$ to the closure of the manifold
            $\overline{\Omega}$, i.e., $\vct{\gamma}_{\vct{x}_0}(t) : \R \rightarrow \overline{\Omega}$.
            $\vct{\gamma}_{\vct{x}}(t) : \R \rightarrow \overline{\Omega}$ such that $\vct{\gamma}_{\vct{x}}(0) = \vct{x}$
            and the limits
            $$
                \lim_{t \to -\infty } \vct{\gamma}_{\vct{x}}(t) ~ \text{ and } ~ 
                \lim_{t \to \infty } \vct{\gamma}_{\vct{x}}(t)
            $$
            exist and converge to \emph{critical-points} of $f$.
        \end{theorem}

        \td{
            TODO:
            - Complete the compact-domain formulation of the gradient flow dynamical system and demonstrate
            that trajectories provably descend the objective and terminate at critical points inside $\overline{\Omega}$.
            -For completeness, connect connect "critical points" to the "stationary points" of the gradient flow ODE and explain
            the relationship between the two and their synonymous use in the literature.
        }
        \newpage
    
        










    % Intro - Optimization Schemes
    \subsection{Optimization Schemes}
        \label{intro:scheme}
        \medskip

        We introduce a \emph{scheme} for solving a \emph{unconstrained optimization problem} based on the \emph{gradient flow}
        dynamical system~\ref{def:grad-flow}.

        \begin{definition}
            An \textbf{optimization scheme} is a one-parameter family of iteration operators
            $T_h : \overline{\Omega} \rightarrow \overline{\Omega}$, indexed by a step size 
            $h \in (0, h_0]$ where $h_0$ is constant, that generates an iterative sequence using the rule
            \begin{equation}
                \label{eq:scheme}
                \vct{x}_{k+1} = T_h(\vct{x}_k) \text{ for } k = 0, 1, 2, \ldots
            \end{equation}
            starting from an initial point $\vct{x}_0 \in \overline{\Omega}$. The scheme is well-defined such that the 
            triplet $(\vct{x}_0, h, T_h)$ satisfy:
            \begin{enumerate}
                \item Consistency: the iteration operator $T_h$ is \emph{consistent} with the \eqref{eq:gradient-flow} ODE
                    if the following limit holds:
                    $$
                        \frac{\left(T_h(\vct{x}) - \vct{x}\right)}{h} + \nabla f(\vct{x}) \rightarrow 0 \text{ as } h \rightarrow 0
                        \text{ for all } \vct{x} \in \overline{\Omega},
                    $$
                    A consistent scheme approximates the continuous gradient flow w/ a local error of
                    $\mathcal{O}(h^{p+1})$ committed at each step $k$; where $p$ is the global order.
                    Consistent schemes reproduce the exact optimality condition in the limit as $h \to 0$.
   
                \item Stability: let $\vct{x}^*$ be a strict local minimizer of $f$. The operator $T_h$ is 
                    \emph{stable}, or \emph{locally contractive}, at $\vct{x}^*$ if there exist constants 
                    $c>0$ and $h_0 \le \frac{2}{L}$, and an open neighborhood $N\subset \overline{\Omega}$ 
                    with $x^*\in N$, such that
                    $$
                        \| T_h(x) - T_h(y) \| \le \bigl(1 - c h \bigr)\, \| x - y \| \quad\forall~x,y\in N,~\forall~ h \in (0, h_0].
                    $$
                    Equivalently, by the mean-value theorem, the Jacobian of $T_h$ must satisfy the following uniform spectral-radius bound
                    $$
                        \rho\!\bigl(D~T_h(\vct{x}^*)\bigr) ~ \le ~ 1 - ch \quad \forall~ h \in(0,h_0]
                    $$
                    where $\rho(\cdot)$ denotes the spectral radius.  In either form, the contraction factor
                    $1 - c h$ lies strictly inside $(0, 1)$, so that the iterates produced by the scheme remain 
                    in $N$ and satisfy
                    $$
                        \|\vct{x}_{k+1} - \vct{x}^*\| \le (1  - ch) \|\vct{x}_k - \vct{x}^*\|,
                    $$
                    implying convergence whenever $h\in(0,h_0]$ is fixed.
            \end{enumerate}
            If a scheme is order-$p$ consistent and stable in a neighborhood of a strict local minimizer,
            then consistent of order $p$ and stable within a neighborhood of a strict local minimizer, then 
            $\vct{x_k} \rightarrow \vct{x}^*$ as $k\to\infty$ and the scheme is said to be \emph{convergent}.
        \end{definition}

        The following iteration operators are examples of schemes for solving
        \eqref{eq:unconstrained-opt}.
        \begin{itemize}
            \item \textbf{Gradient Descent (GD)}: $T_h(\vct{x}) = \vct{x} - h \nabla f(\vct{x})$
            is first-order $(p = 1)$ and contractive when $\nabla^2 f \succeq \mu I \succeq 0$.
            \item \textbf{Newton's Method (NM)}: $T_h(\vct{x}) = \vct{x} - h \nabla^2 f(\vct{x})^{-1} \nabla f(\vct{x})$
            is second-order $(p = 2)$ and contractive when $\nabla^2 f \succeq \mu I \succeq 0$.
            \item \textbf{Trust Region (TR)}: $T_h(\vct{x}) = \vct{x} + \arg \min_{\vct{\tau}} m_{\vct{x}}(\vct{\tau})$
            where $m_{\vct{x}}(\vct{\tau}) = f(\vct{x}) + \langle \nabla f(\vct{x}), \vct{\tau} \rangle +
            \frac{1}{2} \langle \vct{\tau}, \nabla^2 f(\vct{x}) \vct{\tau} \rangle$ is the quadratic approximation
            of $f$ at $\vct{x}$ and $\|\vct{\tau}\| \leq \Delta$ is the trust region constraint.
            \item \textbf{Quasi-Newton (QN)}: $T_h(\vct{x}) = \vct{x} - h B \nabla f(\vct{x})$ where
            $B \approx \nabla^2 f^{-1}(\vct{x})$ is a positive-definite approximation of the Hessian.
            the quasi-newton method is a first-order $(p = 1)$ scheme and contractive when
            $\nabla^2 f \succeq \mu I \succeq 0$.
            \item \textbf{TODO} Make this a table
        \end{itemize}

        \medskip

        \todo{
            TODO: numerics disscusion, introduce the $\eps$ order stationary points.
        }

        \subsubsection*{Analysis of Gradient Descent (GD)}

            \remeber{}{
                TODO: use the following notes and provide an example of the analysis of GD.
                Relate the step choices below to the Wolfe conditions, cite Nocedal and Wright.
            }

            \begin{theorem} Assume $f$ is $\ell$-smooth and $\alpha$-strongly convex and that $\eps > 0$.
                If we iterate the gradient descent \emph{scheme} with $h = h_0 = \frac{1}{\ell}$ held fixed, i.e.,
                $$
                    T_{h}(\vct{x}_k) = \vct{x}_k - \frac{1}{\ell} \nabla f(\vct{x}_k),
                $$
                then $d(x_k, x^*) \leq \eps$ for all $k > K$ where $K$ is chosen to satisfy
                $$
                    \frac{2\ell}{\alpha} \cdot \log\left(\frac{d(x_0, x^*)}{\eps}\right) \leq K.
                $$
            \end{theorem}

            \begin{remark}
                Under \emph{$\ell$-smoothness} and \emph{$\alpha$-strong convexity} assumptions
                in a neighborhood $\Omega$ about $\vct{x}^*$, it may be shown directly from 
                the above theorem above that the \emph{GD scheme} converges linearly to 
                the optimal solution $\vct{x}^*$ at a rate of
                $$
                    \frac{d(\vct{x}_k, \vct{x}^*)}{d(\vct{x}_{k-1}, \vct{x}^*)} \leq 1 - \frac{\alpha}{\ell}
                $$
                where $d(\vct{x}_k, \vct{x}^*)$ is the distance between the current iterate $\vct{x}_k$ and 
                the optimal solution $\vct{x}^*$. The convergence rate is linear in the sense that the 
                distance between the current iterate and the optimal solution decreases by a factor of 
                $1 - \frac{\alpha}{\ell}$ at each iteration. (Ref: TODO)
            \end{remark}

            \begin{remark}
                Convergence to a first-order stationary point trivally
                implies convergence to a $\eps$-first-order stationary point.
                Similarly, convergence to a second-order stationary point trivially
                implies convergence to a $\eps$-second-order stationary point.
            \end{remark}

            \medskip

            \begin{theorem}
                Assume $f$ is $\ell$-smooth, then for any $\eps > 0$, if we iterate
                the GD scheme with $h = h_0 = \frac{1}{\ell}$ held fixed starting from
                $\vct{x}_0 \in \Omega$ where $\Omega$ is a neighborhood of $x^*$,
                then the number of iterations $K$ required to achieve the stopping condition
                $\| \nabla f(\vct{x}_k) \| \leq \eps$ is at most
                $$
                    \left\lceil \frac{\ell}{\eps^{2}} \, (f(\vct{x}_0) - f(\vct{x}^*)) \right\rceil
                $$
            \end{theorem}

            \begin{remark}
                \textbf{TODO and Questions}
                \begin{itemize}
                    \item State how we use theorems in when performing analysis from
                    the results of our experimewts.
                    \item What is the relationship between $\ell$ and $\alpha$?
                    \item In practice do we know how to compute $\ell$ and $\alpha$?
                    \item What is the relationship between $\ell$ and $\rho$?
                    \item In practice do we know how to compute $\ell$ and $\rho$?
                \end{itemize}
            \end{remark}

        \medskip
    

    % Intro - Benchmark Problems
    \subsection{Benchmark Problems}
        \label{intro:problems}
        We select a subset of the \texttt{OptimizationProblems.jl}~\cite{OptimizationProblems}
        Julia package that support automatic differentiation (AD) natively through operator
        overloading. (TODO: Cite ForwardDiff.jl, Flux.jl, etc.) Each problem is implemented
        as \texttt{ADNLPModel} instance, which is a wrapper around the \texttt{NLPModel} interface
        whose backend AD engine is configurable to support forward-mode or reverse-mode.



    

    % ------------------ Theory
    
\section{Theory}
    Note that $\Omega$ is a bounded subset of $\R^n$, so its closure
    $\overline{\Omega} = \Omega \cup \partial \Omega$ is a compact subset
    of $\R^n$, by the Heine-Borel Theorem.
    Also, the boundary $\partial \Omega$ is sufficiently smooth,
    so we can apply the theory of smooth manifolds.
    The closure $\overline{\Omega}$ is a compact subset of $\R^n$ and
    is a smooth manifold with boundary $\partial \Omega$.
    The interior $\Omega$ is an open subset of $\R^n$ and is a smooth manifold.

    \td{Move the above bit to 1.3}

\begin{definition}
    \textcolor{blue}{
        A point $\vct{x}^* \in \Omega$ is a \textbf{critical point} of $f$ if the
        differentiable map $df_p: T_p \Omega \rightarrow \R$ is zero. (Here
        $T_p \Omega$ is a tangent space of the Manifold $M$ at $p$.) 
        The set of critical points of $f$ is denoted by $\text{crit}(f)$.
    }
\end{definition}

\begin{definition}
    \textcolor{blue}{
        A point $\vct{x}^* \in \Omega$ is a \textbf{non-degenerate critical point} of $f$ if
        the Hessian $H_p f$ is non-singular.
    }
\end{definition}

\begin{definition}
    \textcolor{blue}{
        The \textbf{index} of a \emph{non-degenerate critical point} $\vct{x}^*$ is defined to be
        the dimension of the negative eigenspace of the Hessian $H_p f$.
        \begin{itemize}
            \item local minima at $\vct{x}^*$ have index $0$.
            \item local maxima at $\vct{x}^*$ have index $n$.
            \item saddle points at $\vct{x}^*$ have index $k$ where $0 < k < n$.
        \end{itemize}
        We reserve the integers $c_0, c_1, \dots, c_i, \dots, c_n$ to denote the number of
        critical points of index $i$.
    }
\end{definition}

\begin{definition}
    \textcolor{blue}{
        A \textbf{Morse function} is a smooth function $f : \Omega \rightarrow \R$ such that
        all critical points of $f$ are non-degenerate.
    }
\end{definition}

\subsection{Morse Theory in a Metric Space}

\begin{theorem}
    \textcolor{Blue}{
        Let $f$ be a Morse function on $\Omega$, then the Euler characteristic of $\Omega$ is
        given by
        $$
            \chi(\Omega) = \sum_{i=0}^n (-1)^i c_i
        $$
        where $c_i$ is the number of critical points of index $i$.
    }
\end{theorem}

\begin{remark}
    The Euler characteristic $\chi(\Omega)$ is a topological invariant of the manifold $\Omega$
    and is independent of the choice of Morse function $f$.
    The Euler characteristic is a measure of the "shape" of the manifold and can be used to
    distinguish between different topological spaces.
    The Euler characteristic may be defined by the alternating sum of the Betti numbers
    $b_i$ of the manifold $\Omega$
    $$
        \chi(\Omega) = \sum_{i=0}^n (-1)^i b_i
    $$
    where $b_i$ is the $i$-th Betti number of the manifold $\Omega$.
\end{remark}

\begin{theorem}
    \textcolor{ForestGreen}{
        (Sard's theorem) Let $f$ be a Morse function on $\Omega$, then
        the image $f(\text{crit}(f))$ has Lebesque measure zero in $\R$.
    }
\end{theorem}

\begin{remark}
    We state a particular instance of Sard's theorem for continuous scalar-valued functions $f$,
    which was first proved by Anothony P. Morse in 1939.
    The theorem asserts that the image of the critical points of a Morse function is a set 
    of measure zero in $\R$. This means that the critical points of a Morse function are "rare" in the sense that they
    do not form a dense subset of the manifold $\Omega$.
    Consequently, selecting $\vct{x} \in \Omega$ at random will almost never yeild a critical
    point of $f$.
\end{remark}

\begin{remark}
    The property that $\vct{x}^* \in \Omega$ being a \emph{critical point} of a Morse function $f$ is
    not dependent of the metric of $\Omega \subset \R^n$ (and consequently, the norm induced by the metric)
\end{remark}



    % \subsection{Signed Distance Function}
    % \textcolor{Red}{
    %     TODO: Add signed distance function theory and cite ref
    %     Stanley Osher and Ronald Fedkiw, Level Set Methods and Dynamic
    %     Implicit Surfaces, Springer, 2003.
    % }

    % \subsection{Time Marching Level Set Methods}
    % \textcolor{Red}{
    %     More Osher and Fedkiw, Level Set Methods and Dynamic
    %     Implicit Surfaces, Springer, 2003.
    % }

    % \subsection{Saard's Theorem and the Morse Lemma}

    % \section{Alternative Approaches}
    % \subsection{Level Set Methods}
    % \textcolor{Red}{
    %     TODO: Explain how level set methods would allow us
    %     to classify the critical point structure of $f$.
    %     And their role in low-dimensional topology.
    %     Explain curse of dimensionality and how level set methods
    %     can be used to classify the critical point structure of $f$.
    % }
    
    % \subsection{Discontinuous Galerkin Methods / Transportation Methods}
    % \textcolor{Red}{
    %     High dimensional sparse grid methods
    % }


    % \subsection{Machine Learning}
    % \textcolor{Red}{
    %     TODO: Explain how machine learning can be used to classify
    %     the critical point structure of $f$.
    %     Note, this is in combination with the level set methods
    %     minimal surface methods approximation obtained via
    %     dimensionality reduction.
    %     \textbf{TODO:} Interesting to think about learning the critical
    %     point structure of $f$ via a neural network in small dimensions..
    %     Ecspecially trace data, to help us better understand why some
    %     Quasi-Newton methods work better than others.
    % }

    % \subsection*{Motivation}
    % \medskip
    % \textcolor{Red}{
    % We construct a test set of standard unconstrained optimization problems with orgins
    % in the \emph{Constrained and Unconstrained Testing Environment} \cite{CUTE} (CUTE).
    % The latest evolution of \cite{CUTE} is \emph{CUTEest} \cite{CUTEst}, which is implemented
    % in native Julia code within the \texttt{OptimizationProblems.jl} \cite{OptimizationProblems}
    % Julia package. The \texttt{OptimizationProblems.jl} package is a collection of
    % optimization problems, we are interested in unconstrained problems that support
    % automatic differentiation (AD) and allow for variable dimensions.   
    % Our motivation is to design a procedure to classify the 
    % \emph{critical-point} structure of the objective function $f$.\\
    % The \texttt{OptimizationProblems.jl} Julia Langauge package exposes a collection
    % of unconstrained minimization test problems written in native Julia code. 
    % We select a subset of these problems that support Automatic Differentiation (AD)
    % and allow for variable dimensions.
    % }

% ------------------ APPENDIX
\newpage
\section{Apendix}

\subsection*{Notation}
    We assume the following notation throughout\
    \begin{itemize}
        \label{notation}
        \item $\| \cdot \|$ denotes the usual $\ell_2$ norm for vectors $\vct{x}$ in $\mathbb{R}^n$ and $p = 2$ norm
        for matrices in $\mathbb{R}^{n \times m}$. i.e.,
        \begin{flalign*}
            \|x\| & := \left(\sum_i x_i^2\right)^{1/2} \\
                \|A\| & := (\lambda_{\max}(A^\top A))^{1/2}
        = \max(\sigma(A))
        \end{flalign*}
        \item $\sigma(A) := \{\text{singular values of } A\}$.
        \item $A \in \mathbb{R}^{n\times n}$ $\implies ~ \sigma(A) = \{\text{eigenvalues of } A \text{ (i.e. spectrum)}\}$
        \item $\sigma_{\max}(A) := \max(\sigma(A))$ and $\sigma_{\min}(A) := \min(\sigma(A))$.
        \item $ A \in \mathbb{R}^{n\times n}$  $\implies \lambda_{\max}(A) := \sigma_{\max}(A)$ and $\lambda_{\min}(A) = \sigma_{\min}(A) $
        \item $\langle \cdot, \cdot \rangle$ denotes the usual inner product on $\mathbb{R}^n$, i.e.,
        $$
            \langle \vct{x}, \vct{y} \rangle := \vct{x}^\top \vct{y} = \sum_{i=1}^n \vct{x}_i \vct{y}_i = \| \vct{x} \| \|\vct{y}\| \cos(\theta)
        $$
        where $\theta$ is the angle between $\vct{x}$ and $\vct{y}$.
        \item $\mathcal{B}_r(\vct{x}) := \{ \vct{y} \in \mathbb{R}^n : \|\vct{y} - \vct{x}\| < r \}$ is the open ball of 
        radius $r$ centered at $\vct{x} \in \R^n$.
        \item $\text{crit}(f) = \{ \vct{x}^* \in \R^n : \nabla f(\vct{x}^*) = 0 \}$ is the set of critical points of $f$.
    \end{itemize}


\subsection*{Definitions}
    \label{sec:definitions}
    
    % Lipschitz
    \begin{definition}
        $f$ is \textbf{$L$-Lipschitz} if $\forall ~ \vct{x_1},\vct{x_2}$
        $$
        \; \exists~ L \geq 0 ~:~  \| f(\vct{x_1}) - f(\vct{x_2})\| 
        \leq L\|\vct{x_1} - \vct{x_2}\|
        $$
    \end{definition}

    % L-Lipschitz gradient
    \begin{definition} 
        $f$ has \textbf{$\ell$-Lipschitz gradient}, or, $f$ is \textbf{$\ell$-smooth} if
        $\forall ~ \vct{x_1},\vct{x_2}$
        $$
            \exists~ \ell \geq 0 ~:~ \|\nabla f(\vct{x_1}) - \nabla f(\vct{x_2})\| 
            \leq \ell\|\vct{x_1} - \vct{x_2}\|
        $$
    \end{definition}

    % Rho Lipschitz Hessian
    \begin{definition}
        $f$ has \textbf{$\rho$-Lipschitz Hessian} if $\forall ~ \vct{x_1},\vct{x_2}$
        $$
            \; \exists~ \rho \geq 0 ~:~ \|\nabla^2 f(\vct{x_1}) - \nabla^2 f(\vct{x_2})\| 
            \leq \rho\|\vct{x_1} - \vct{x_2}\|
        $$
    \end{definition}

    % Convexity
    \begin{definition}
        $f$ is \textbf{convex} if $\forall ~ \vct{x_1},\vct{x_2}$
        \begin{flalign*}
            f(\vct{x_2}) & \geq f(\vct{x_1}) + \langle  \vct{x_2} - \vct{x_1}, \nabla f(\vct{x_1}) \rangle\\
                        & = f(\vct{x_1}) + \nabla f(\vct{x_1})^T(\vct{x_2} - \vct{x_1}) 
        \end{flalign*}
    \end{definition}

    % Strictly Convex
    \begin{definition}
        $f$ is \textbf{strictly convex} if
        \begin{flalign*}
            \exists~ \mu > 0 ~ : ~ & \nabla^2 f \succeq \mu I \\ 
            & \iff \lambda_{\min}(\nabla^2 f) \geq \mu > 0 
        \end{flalign*}
    \end{definition}

    % Alpha strong convexity
    \begin{definition}
        $f$ is \textbf{$\alpha$-strongly convex} if $\forall ~ \vct{x_1},\vct{x_2} \exists~ \alpha > 0$ s.t.
        \begin{flalign*}
            & f(\vct{x_2}) \geq f(\vct{x_1}) + \langle \nabla f(\vct{x_1}), \vct{x_2} - \vct{x_1} \rangle + 
            \frac{\alpha}{2}\|\vct{x_2} - \vct{x_1} \|^2\\
            & \iff \lambda_{\min}(\nabla^2 f (\vct{x})) \geq - \alpha. 
        \end{flalign*}
    \end{definition}

    % First-order stationary point
    \begin{definition}
        $\vct{x^*}$ is a \textbf{first-order stationary point}
        if $\| \nabla f(x^*) \| = 0$.
    \end{definition}

    % Eps-first-order stationary point
    \begin{definition}
        $\vct{x^*}$ is an \textbf{$\eps$-first-order stationary point} 
        if $\|\nabla f(x^*)\| \leq \eps$. 
    \end{definition}

    % Second-order stationary point
    \begin{definition}
        $\vct{x}^* \in \R^n$ is a \textbf{second-order stationary point} if 
        $\| \nabla f(x^*) \| = 0\text{ and } \nabla^2 f(x^*) \succeq 0$.
    \end{definition}

    % Eps-second-order stationary point
    \begin{definition}
        if $f$ has $\rho$-Lipschitz Hessian, $\vct{x}^* \in \R^n$ is a
        \textbf{$\eps$-second-order stationary point} if 
        $$
            \| \nabla f(x^*) \| \leq \eps \text{ and }\nabla^2 f(x^*) \succeq -\sqrt{\rho \eps}
        $$
        \begin{remark}
            Note that the Hessian is not required to be positive definite, 
            but it is required to have a small eigenvalue. 
        \end{remark}
    \end{definition}


    \begin{definition}
        \textcolor{blue}{
            A point $\vct{x^*} \in \Omega$ is a \textbf{critical point} of $f$ if the
            differentiable map $df_p: T_p \Omega \rightarrow \R$ is zero. (Here
            $T_p \Omega$ is a tangent space of the Manifold $M$ at $p$.) 
            The set of critical points of $f$ is denoted by $\text{crit}(f)$.
        }
    \end{definition}

    \begin{definition}
        A point $\vct{x^*} \in \Omega$ is a \textbf{non-degenerate critical point} of $f$ if
        the Hessian $H_p f$ is non-singular.
    \end{definition}

    \begin{definition}
        The \textbf{index} of a \emph{non-degenerate critical point} $\vct{x^*}$ is defined to be
        the dimension of the negative eigenspace of the Hessian $H_p f$.
        \begin{itemize}
            \item local minima at $\vct{x^*}$ have index $0$.
            \item local maxima at $\vct{x^*}$ have index $n$.
            \item saddle points at $\vct{x^*}$ have index $k$ where $0 < k < n$.
        \end{itemize}
        We reserve the integers $c_0, c_1, \dots, c_i, \dots, c_n$ to denote the number of
        critical points of index $i$.
    \end{definition}

    \begin{remark}
        For each objective function $f$ we are interested in determining the
        critical points of $f$ 
    \end{remark}

    \begin{remark}
        The \textbf{Morse function} is a smooth function $f : \Omega \rightarrow \R$ such that
        all critical points of $f$ are non-degenerate.
    \end{remark}


% ------------------ TEST PROBLEMS
\subsection{Test Problems}

\textbf{Generalized Rosenbrock function:}
    \[
    f(x) = \sum_{i=1}^{n-1} \left( c(x_{i+1} - x_i^2)^2 + (1 - x_i)^2 \right),
    \]
    \[
    x_0 = [-1.2, 1, -1.2, 1, \dots, -1.2, 1], \quad c = 100.
    \]



% ------------------- Code Listings
\subsection{Code Listings}

Below are the code listings for the experiments conducted in this report.

\lstset{language=julia}
\begin{lstlisting}[language=julia, caption={Algorithm 16.5}]
   # TODO: Add Code Listings
\end{lstlisting}
\newpage


% ------------------ BIBLIOGRAPHY
\bibliographystyle{alpha}
\bibliography{../references}

\end{document}