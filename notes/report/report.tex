\documentclass[10pt]{article}

\input{../preamble.tex}

% ----------------- TITLE
\title{Classification of Smooth Unconstrained Optimization Problems}
\author{Daniel Henderson}
\date{\today}

% ------------------ DOCUMENT
\begin{document}
\maketitle

% ------------------ ABSTRACT
\begin{abstract}
    \noindent
    \textcolor{Red}{
    Analysis of standard set of unconstrained minimization problems
    of a smooth objective function $f: \R^n \rightarrow \R$.
    Our test problems are selected from the \texttt{OptimizationProblems.jl}
    \cite{OptimizationProblems} Julia package.
    We investigate the local convexity of eac problem by sampling
    the Hessian $\nabla^2 f(x)$ at a set of random points $x$ in
    the neighborhood of a strict local minimizer $x^*$.
    We start with an introduction and survey of the Literature,
    followed by a description of the test problems and the sampling
    procedure. We then present the results of our sampling experiment
    and a discussion of the results.
    }
    \noindent\textbf{Keywords:} Julia, Optimization, Benchmarking, Automatic Differentiation
\end{abstract}



% ------------------ TABLE OF CONTENTS
\tableofcontents
\newpage

% ------------------ INTRODUCTION
\section{Introduction}
    \textcolor{Red}{
    Our aim is to investigate the local convexity of standard
    test problems from the Constrained and Unconstrained Optimization Test
    Environment (CUTE).We start with a brief review of continuous
    smooth optimization problems.
    We then present a set of test problems selected from the
    \texttt{OptimizationProblems.jl} \cite{OptimizationProblems}
    Julia package. Next, we provide a theoritical overview on 
    the classification of an objective functions critical point
    structure. We then present a sampling procedure to classify
    the \emph{critical-point} structure of the objective function
    $f: \R^n \rightarrow \R$ within local neighborhoods of a strict
    local minimizer $x^*$. We present a set of test problems
    selected from the \texttt{OptimizationProblems.jl} \cite{OptimizationProblems}
    Julia package.
    }
    \medskip

    % State Notation
    \subsection{Preliminaries}    
    \textcolor{Red}{
        We assume the notation defined in the Appendix~\ref{notation}.
    }
    \medskip

    % State Problem
    \subsection{Problem}
    The general form of an \textbf{unconstrained optimization problem} is
    \begin{flalign}
        \min_{\vct{x} \in \R^n} f(\vct{x})
        \label{eq:unconstrained-opt}
    \end{flalign}
    where $\vct{x} \in \R^n$ is the optimization variable and $f: \R^n \rightarrow \R$ is
    a sufficiently smooth objective function.
    We requre that $f \in C^2(\overline{\Omega}; \R)$, where
    the domain $\Omega \subset \R^n$ is a bounded open subset of $\R^n$; i.e.,
    $f$ is twice continuously differentiable at every point $\vct{x} \in \Omega$ and
    it's derivatives may be continuously extended to the domains boundary $\partial \Omega$.
    The \textbf{optimal solution} and \textbf{optimal value} 
    of \eqref{eq:unconstrained-opt} is denoted by
    $$
        \vct{x}^* = \mathop{\mathrm{arg\,min}}_{\vct{x} \in K} f(\vct{x})
        \quad \text{ and } \quad
        \vct{f}^* = \mathop{\mathrm{min}}_{\vct{x} \in K} f(\vct{x}),
    $$
    where $\vct{x}^*$ satisfies the \emph{first-order} optimality condition that $\nabla f(\vct{x}^*) = 0$.\\

    A \emph{critical-point} is a point satisfying the \emph{first-order} optimality condition.
    Let $\vct{x^*}$ denote a critical-point, and note that $\vct{x}^*$ is either a local minimizer, 
    local maximizer, or a saddle point of $f$.
    The \emph{second-order} optimality condition asserts that $\nabla^2 f(\vct{x}^*) \succeq 0$.
    A \emph{strict local minimizer} $\vct{x}^*$ of $f$ is a \emph{critical-point} such that
    $\nabla^2 f(\vct{x}^*) \succ 0$, i.e. the Hessian of $f$ is positive definite at $\vct{x}^*$.
    In general, we seek an \emph{optimal value} $\vct{x}^*$ satisfying the \emph{second-order}
    optimality condition that $\nabla^2 f(\vct{x}^*)$ is semi-positive definite.
        As defined in \cite{NocedalAndWright06}, we say an \emph{optimal-value} $\vct{x}^*$ is
    \begin{itemize}
        \item \emph{global minimizer} if $f(\vct{x}^*) \leq f(\vct{x}) ~ \forall x \in \R^n$
        \item \emph{local minimizer} if there is a neighborhood $N$ of $\vct{x}^*$
        such that $f(\vct{x}^*) \leq f(\vct{x})$ for all $\vct{x} \in N$
        \item \emph{strict local minimizer} if there is a neighborhood $N$ of $\vct{x}^*$
        such that $f(\vct{x}^*) < f(x)$ for all $x \in N / \{ \vct{x}^*\}$
        \item \emph{isolated local minimizer} if there is a neighborhood $N$ of $\vct{x}^*$
        such that $\vct{x}^*$ is the only local minimizer in $N$
        \item \emph{stationary point} if $\nabla f(x^*) = 0$ (any local minimizer is a stationary point)
    \end{itemize}
    \medskip
    The \emph{first-order} optimiality condition asserts $\nabla f(x^*) = 0$,
    which is equivalent to saying $\vct{x}^*$ is a \emph{stationary point}.
    The second order optimality conditions assert that the Hessian of $f$
    at $\vct{x}^*$ is semi-positive definite, which is
    equivalent to saying that $\vct{x}^*$ \emph{local minimizer}.
    Note, if $\nabla^2 f(x^*)$ is positive definite, we say that $\vct{x}^*$ is a \emph{strict local minimizer}.
    So we arrive at the following necessary and sufficient conditions for a stationary point $\vct{x}^*$ 
    to be a strict local minimizer
    \begin{itemize}
        \item \emph{Necessary:} $\nabla f(x^*) = 0$ and $\nabla^2 f(x^*) \succeq 0 $ (positive semi-definite)
        \item \emph{Sufficient:} $\nabla f(x^*) = 0$ and $\nabla^2 f(x^*) \succ 0$ (positive definite).
    \end{itemize}
    Consequently, \emph{strict local minimizers} are points where the objective function is concave up.
    \medskip

    \subsection{Continous Models}
    
    Let us start by defining a \emph{continuous model} for the
    general form of problem \ref{eq:unconstrained-opt}.
    
    \begin{definition}
        Let the \textbf{gradient flow} of $f$ be a solution to the dynamical system defined as
        \begin{flalign}
            \vct{\gamma}'(t) = -\nabla f(\vct{\gamma}(t)) 
        \label{eq:gradient-flow}
        \end{flalign}
        where the evolution of our phase space is driven by the negative gradient of $f$.
        A \textbf{gradient flow line} of $f$ is an integral curve 
        $\vct{\gamma} : [0, \infty) \rightarrow \Omega$ satisfying the above evolutionary 
        ordinary-differential equation subject to $\vct{\gamma}(0) = \vct{x_0}$.
    \end{definition}

    \medskip

    Suppose that our initial point is a stationary-point, that is, 
    $\vct{x_0} = \vct{x^*}$. Then we aim to show the constant \emph{gradient flow line} defined as
    $\vct{\gamma}(t) = \vct{x}^* ~ \forall ~ t \in [0, \infty)$ satisfies $\eqref{eq:gradient-flow}$.
    Notice that it ought to be that $\vct{\gamma}'(t) = \vct{0}$ for all time $t$ and
    since $\vct{x}^*$ is a stationary point, we have that $\nabla f(\vct{x}^*) = \vct{0}$.
    Therefore, we have that
    \begin{flalign*}
        \vct{\gamma}'(t) & = \vct{0} = -\nabla f(\vct{\gamma}(t)) = -\nabla f(\vct{x}^*) = \vct{0} ~\text{ and }~ \vct{\gamma}(t) = \vct{x}^* ~ \forall ~ t \in [0, t_f]
    \end{flalign*}
    So we've shown that $\vct{\gamma}(t)$ is a constant flow line of $f$.
    Consequently, by the uniqueness of solutions for ordinary differential
    equations, if any flow line contains a \emph{first-order} critical point $\vct{x}^*$,
    it must be a constant flow line.

    \textcolor{Red}{
    \begin{lemma}
        The function $f: \omega \rightarrow \R$ is nonincreasing
        along any flow-line $\vct{\gamma}(t)$ and strictly decreasing
        along flow lines not containing a critical point $x^*$.
    \end{lemma}
    \begin{proof}
        Let $\vct{\gamma} : [0, t_f] \rightarrow \Omega$ be a flow line.
        Consider the composition $f \circ \vct{\gamma} : [0, t_f] \rightarrow \R$,
        its derivative is
        \begin{flalign*}
            \dfrac{d}{dt}\big( f(\vct{\gamma}(t)) \big) & =
                \langle \nabla_{\vct{\gamma(t)}}(f), \dfrac{d\vct{\gamma}}{dt} \rangle \\
                 & = \langle \nabla_{\vct{\gamma(t)}}(f), - \nabla_{\gamma(t)}(f) \rangle \\
                 & = - \langle \nabla_{\vct{\gamma(t)}}(f), \nabla_{\gamma(t)}(f) \rangle \\
                 & = - | \nabla_{\vct{\gamma(t)}}(f) |^2 \\
                 & \leq 0
        \end{flalign*}
        Therefore, $f'(\vct{\gamma(t)}) = 0$ iff $\vct{\gamma}(t)$ is on a critical point
        of $f$. In particular, if $\vct{\gamma}(t)$ does not contain in its image
        a critical point of $f$, then the above inequality implies that
        $f$ is strictly decreasing along the integral curve $\vct{\gamma(t)}$.
    \end{proof}
    \begin{theorem}
        For all $\vct{x}$ in the closed manifold $\overline{\Omega}$, there exists
        uniquely $\vct{\gamma}_{\vct{x}}(t) : \R \rightarrow \overline{\Omega}$
        such that $\vct{\gamma}_{\vct{x}}(0) = \vct{x}$
        and the limits
        $$
            \lim_{t \to -\infty } \vct{\gamma}_{\vct{x}}(t) ~ \text{ and } ~ 
            \lim_{t \to \infty } \vct{\gamma}_{\vct{x}}(t)
        $$
        exist and converge to \emph{critical-points} of $f$.
    \end{theorem}         
    Now it may be shown that the flow map operator $T : \overline{\Omega} \times \R \rightarrow \overline{\Omega}$
    defined as $T(\vct{x}, t) : =  \vct{\gamma}_{\vct{x}}(t)$. This implies theoretically that we may
    perform analysis of our phase space as constructed by a smooth union of integral curves.
    Consequently, by our previous lemma, if the flow map $T$ contains a critical point $\vct{x}^*$ then
    it ought to be that $T(\vct{x}^*, t) \equiv \text{ const } ~ \forall t$, otherwise,
    $T$ is descending for all of time.
    }



    % Scheme for solving unconstrained optimization problem
    \subsection{Optimization Schemes}
    A \emph{scheme} for solving $\eqref{eq:unconstrained-opt}$ is a numerical
    method for approximating the optimal solution $\vct{x}^*$. 

    \textcolor{Red}{
        % Scheme for solving unconstrained optimization problem
        \begin{definition}
            A \textbf{scheme} for solving a general form \emph{unconstrained optimization problem}
            is a one-parameter family of iteration operators:
            $$
                T_h : \R^n \rightarrow \R^n ~ \text{ where } ~\vct{x}_{k+1} = T_h(\vct{x}_k), ~ h \in (0, h_0]
            $$
            where $h_0$ is a constant and $h$ is the step size. The scheme is well-defined such that
            the triplet $(\vct{x}_0, h, T_h)$ satisfy
            \begin{enumerate}
                \item \emph{Consistency}: $\forall \vct{x} \in K$ 
                $$
                    \left( T_h(\vct{x}) - \vct{x} \right) h^{-1} + \nabla f(\vct{x}) \rightarrow 0 \text{ as } h \rightarrow 0.
                $$
                implying that a single step approximates the continous gradient flow w/ local error
                $\mathcal{O}(h^{p+1})$ where $p$ is the global order of the scheme.
                \item \emph{Stability}: $\exists~ c > 0, h_0 > 0 ~:~ \forall h \in (0, h_0]$,
                and for all $\vct{x_1}, \vct{x_2}$ in a \emph{neighborhood $N \subset K$}
                around an \emph{optimal solution} $\vct{x}^*$
                $$
                    \| T_h(\vct{x_1}) - T_h(\vct{x_2}) \| \leq (1 - ch) \| \vct{x_1} - \vct{x_2} \|
                $$
                where $c$ is a constant that depends on the scheme and $h$ is the step size.
                Or, equivalently, the scheme is \emph{stable} if
                $\exists~ c > 0, h_0 > 0 ~:~ \forall h \in (0, h_0]$,
                and for all $\vct{x}$ in a \emph{neighborhood} about $\vct{x}^*$,
                each step results in a strict decrease of by atleast a factor of $1 - ch$, i.e.,
                $$
                \| J(T_h(\vct{x})) \| \leq (1 - ch)
                $$
                where $J(T_h(\vct{x}))$ is the Jacobian of the scheme.
                \item \emph{Convergence}: $\forall~ \vct{x_0} \in N \subset K$, s.t. $N$ is some neighborhood
                around a strict minimizer $x^*$. and $\forall \eps > 0$ 
                $$
                    \exists~ K \in \mathbb{N} ~:~ \forall k ~ > K, \vct{x}_k \in N \text{ and } 
                    d(\vct{x}_k, \vct{x}^*) \leq \eps.
                $$
            \end{enumerate}
        \end{definition}
        The following iteration operators are examples of schemes for solving
        \eqref{eq:unconstrained-opt}.
        \begin{itemize}
            \item \textbf{Gradient Descent (GD)}: $T_h(\vct{x}) = \vct{x} - h \nabla f(\vct{x})$
            is first-order $(p = 1)$ and contractive when $\nabla^2 f \succeq \mu I \succeq 0$.
            \item \textbf{Newton's Method (NM)}: $T_h(\vct{x}) = \vct{x} - h \nabla^2 f(\vct{x})^{-1} \nabla f(\vct{x})$
            is second-order $(p = 2)$ and contractive when $\nabla^2 f \succeq \mu I \succeq 0$.
            \item \textbf{Trust Region (TR)}: $T_h(\vct{x}) = \vct{x} + \arg \min_{\vct{\tau}} m_{\vct{x}}(\vct{\tau})$
            where $m_{\vct{x}}(\vct{\tau}) = f(\vct{x}) + \langle \nabla f(\vct{x}), \vct{\tau} \rangle +
            \frac{1}{2} \langle \vct{\tau}, \nabla^2 f(\vct{x}) \vct{\tau} \rangle$ is the quadratic approximation
            of $f$ at $\vct{x}$ and $\|\vct{\tau}\| \leq \Delta$ is the trust region constraint.
            \item \textbf{Quasi-Newton (QN)}: $T_h(\vct{x}) = \vct{x} - h B \nabla f(\vct{x})$ where
            $B \approx \nabla^2 f^{-1}(\vct{x})$ is a positive-definite approximation of the Hessian.
            the quasi-newton method is a first-order $(p = 1)$ scheme and contractive when
            $\nabla^2 f \succeq \mu I \succeq 0$.
            \item \textbf{TODO} Make this a table
        \end{itemize}
    }

    \medskip

    % ------------------ Theory
    \section{Theory}
    
    Note that $\Omega$ is a bounded subset of $\R^n$, so its closure
    $\overline{\Omega} = \Omega \cup \partial \Omega$ is a compact subset
    of $\R^n$, by the Heine-Borel Theorem.
    Also, the boundary $\partial \Omega$ is sufficiently smooth,
    so we can apply the theory of smooth manifolds.
    The closure $\overline{\Omega}$ is a compact subset of $\R^n$ and
    is a smooth manifold with boundary $\partial \Omega$.
    The interior $\Omega$ is an open subset of $\R^n$ and is a smooth manifold.

    \begin{definition}
        \textcolor{blue}{
            A point $\vct{x^*} \in \Omega$ is a \textbf{critical point} of $f$ if the
            differentiable map $df_p: T_p \Omega \rightarrow \R$ is zero. (Here
            $T_p \Omega$ is a tangent space of the Manifold $M$ at $p$.) 
            The set of critical points of $f$ is denoted by $\text{crit}(f)$.
        }
    \end{definition}

    \begin{definition}
        \textcolor{blue}{
            A point $\vct{x^*} \in \Omega$ is a \textbf{non-degenerate critical point} of $f$ if
            the Hessian $H_p f$ is non-singular.
        }
    \end{definition}

    \begin{definition}
        \textcolor{blue}{
            The \textbf{index} of a \emph{non-degenerate critical point} $\vct{x^*}$ is defined to be
            the dimension of the negative eigenspace of the Hessian $H_p f$.
            \begin{itemize}
                \item local minima at $\vct{x^*}$ have index $0$.
                \item local maxima at $\vct{x^*}$ have index $n$.
                \item saddle points at $\vct{x^*}$ have index $k$ where $0 < k < n$.
            \end{itemize}
            We reserve the integers $c_0, c_1, \dots, c_i, \dots, c_n$ to denote the number of
            critical points of index $i$.
        }
    \end{definition}

    \begin{definition}
        \textcolor{blue}{
            A \textbf{Morse function} is a smooth function $f : \Omega \rightarrow \R$ such that
            all critical points of $f$ are non-degenerate.
        }
    \end{definition}

    \subsection{Morse Theory in a Metric Space}

    \begin{theorem}
        \textcolor{Blue}{
            Let $f$ be a Morse function on $\Omega$, then the Euler characteristic of $\Omega$ is
            given by
            $$
                \chi(\Omega) = \sum_{i=0}^n (-1)^i c_i
            $$
            where $c_i$ is the number of critical points of index $i$.
        }
    \end{theorem}

    \begin{remark}
        The Euler characteristic $\chi(\Omega)$ is a topological invariant of the manifold $\Omega$
        and is independent of the choice of Morse function $f$.
        The Euler characteristic is a measure of the "shape" of the manifold and can be used to
        distinguish between different topological spaces.
        The Euler characteristic may be defined by the alternating sum of the Betti numbers
        $b_i$ of the manifold $\Omega$
        $$
            \chi(\Omega) = \sum_{i=0}^n (-1)^i b_i
        $$
        where $b_i$ is the $i$-th Betti number of the manifold $\Omega$.
    \end{remark}

    \begin{theorem}
        \textcolor{ForestGreen}{
            (Sard's theorem) Let $f$ be a Morse function on $\Omega$, then
            the image $f(\text{crit}(f))$ has Lebesque measure zero in $\R$.
        }
    \end{theorem}

    \begin{remark}
        We state a particular instance of Sard's theorem for continous scalar-valued functions $f$,
        which was first proved by Anothony P. Morse in 1939.
        The theorem asserts that the image of the critical points of a Morse function is a set 
        of measure zero in $\R$. This means that the critical points of a Morse function are "rare" in the sense that they
        do not form a dense subset of the manifold $\Omega$.
        Consequently, selecting $\vct{x} \in \Omega$ at random will almost never yeild a critical
        point of $f$.
    \end{remark}

    \begin{remark}
        The property that $\vct{x^*} \in \Omega$ being a \emph{critical point} of a Morse function $f$ is
        not dependent of the metric of $\Omega \subset \R^n$ (and consequently, the norm induced by the metric)
    \end{remark}



    \subsection{Signed Distance Function}
    \textcolor{Red}{
        TODO: Add signed distance function theory and cite ref
        Stanley Osher and Ronald Fedkiw, Level Set Methods and Dynamic
        Implicit Surfaces, Springer, 2003.
    }

    \subsection{Time Marching Level Set Methods}
    \textcolor{Red}{
        More Osher and Fedkiw, Level Set Methods and Dynamic
        Implicit Surfaces, Springer, 2003.
    }

    \subsection{Saard's Theorem and the Morse Lemma}

    \section{Alternative Approaches}
    \subsection{Level Set Methods}
    \textcolor{Red}{
        TODO: Explain how level set methods would allow us
        to classify the critical point structure of $f$.
        And their role in low-dimensional topology.
        Explain curse of dimensionality and how level set methods
        can be used to classify the critical point structure of $f$.
    }
    
    \subsection{Discontinous Galerkin Methods / Transportation Methods}
    \textcolor{Red}{
        High dimensional sparse grid methods
    }


    \subsection{Machine Learning}
    \textcolor{Red}{
        TODO: Explain how machine learning can be used to classify
        the critical point structure of $f$.
        Note, this is in combination with the level set methods
        minimal surface methods approximation obtained via
        dimensionality reduction.
        \textbf{TODO:} Interesting to think about learning the critical
        point structure of $f$ via a neural network in small dimensions..
        Ecspecially trace data, to help us better understand why some
        Quasi-Newton methods work better than others.
    }

    % \subsection*{Motivation}
    % \medskip
    % \textcolor{Red}{
    % We construct a test set of standard unconstrained optimization problems with orgins
    % in the \emph{Constrained and Unconstrained Testing Environment} \cite{CUTE} (CUTE).
    % The latest evolution of \cite{CUTE} is \emph{CUTEest} \cite{CUTEst}, which is implemented
    % in native Julia code within the \texttt{OptimizationProblems.jl} \cite{OptimizationProblems}
    % Julia package. The \texttt{OptimizationProblems.jl} package is a collection of
    % optimization problems, we are interested in unconstrained problems that support
    % automatic differentiation (AD) and allow for variable dimensions.   
    % Our motivation is to design a procedure to classify the 
    % \emph{critical-point} structure of the objective function $f$.\\
    % The \texttt{OptimizationProblems.jl} Julia Langauge package exposes a collection
    % of unconstrained minimization test problems written in native Julia code. 
    % We select a subset of these problems that support Automatic Differentiation (AD)
    % and allow for variable dimensions.
    % }

% ------------------ APPENDIX
\newpage
\section{Apendix}

\subsection*{Notation}
    We will assume the following notation throughout \textbf{TODO: Add to Appendix}\
    \textcolor{Black}{
        \begin{itemize}
            \item $\| \cdot \|$ denotes the usual $\ell_2$ norm for vectors $\vct{x}$ in $\mathbb{R}^n$ and $p = 2$ norm
            for matrices in $\mathbb{R}^{n \times m}$. i.e.,
            \begin{flalign*}
                \|x\| & := \left(\sum_i x_i^2\right)^{1/2} \\
                 \|A\| & := (\lambda_{\max}(A^\top A))^{1/2}
            = \max(\sigma(A))
            \end{flalign*}
            \item $\sigma(A) := \{\text{singular values of } A\}$.
            \item $A \in \mathbb{R}^{n\times n}$ $\implies ~ \sigma(A) = \{\text{eigenvalues of } A \text{ (i.e. spectrum)}\}$
            \item $\sigma_{\max}(A) := \max(\sigma(A))$ and $\sigma_{\min}(A) := \min(\sigma(A))$.
            \item $ A \in \mathbb{R}^{n\times n}$  $\implies \lambda_{\max}(A) := \sigma_{\max}(A)$ and $\lambda_{\min}(A) = \sigma_{\min}(A) $
            \item $\langle \cdot, \cdot \rangle$ denotes the usual inner product on $\mathbb{R}^n$, i.e.,
            $$
                \langle \vct{x}, \vct{y} \rangle := \vct{x}^\top \vct{y} = \sum_{i=1}^n \vct{x}_i \vct{y}_i = \| \vct{x} \| \|\vct{y}\| \cos(\theta)
            $$
            where $\theta$ is the angle between $\vct{x}$ and $\vct{y}$.
            \item $\mathcal{B}_r(\vct{x}) := \{ \vct{y} \in \mathbb{R}^n : \|\vct{y} - \vct{x}\| < r \}$ is the open ball of radius $r$ centered at $\vct{x} \in \R^n$.
            \item $\text{crit}(f) = \{ \vct{x}^* \in \R^n : \nabla f(\vct{x}^*) = 0 \}$ is the set of critical points of $f$.
        \end{itemize}
    }






\newpage
\subsection{Test Problems}
\newpage




\subsection{Code Listings}
Below are the code listings for the experiments conducted in this report.
\lstset{language=julia}
\begin{lstlisting}[language=julia, caption={Algorithm 16.5}]
    # TODO: Add Code
\end{lstlisting}
\newpage





\bibliographystyle{alpha}
\bibliography{../references}

\end{document}