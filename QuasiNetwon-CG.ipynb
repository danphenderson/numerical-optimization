{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust Region CG-Newton Method\n",
    "\n",
    "Before we begin our disscusion of conjugate gradient quasi-newton trust regions methods we begin with a newton direction in CG routine that was introduced by Steihaug.\n",
    "The use of a CG scheme is in our trust-region subproblem, namaley, when we try to minimize our model function over a region defined by our TR radius $\\Delta_k$.\n",
    "Specifically, we use a newton CG to solve:  \n",
    "\n",
    "$$ \\min_{p \\in \\mathbb{R}} m_k(p) = f_k + g_k^T p + \\frac12 p^T B_k p ~ : ||p|| \\leq \\Delta_K, $$\n",
    "\n",
    "where $g_k = \\nabla f(x_k)$ and $B_k = \\nabla^2 f(x_k)$ (the Newton direction). This problem is solved in `cg_steihaug` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cg_steihaug (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra, ForwardDiff\n",
    "\n",
    "function cg_steihaug(f, x, Δ, ϵ) #Alg 7.2\n",
    "    ∇f = dx -> ForwardDiff.gradient(f, dx)\n",
    "    ∇²f = dx -> ForwardDiff.hessian(f, dx)\n",
    "    print(\"hi\")\n",
    "    d = -∇f(x)\n",
    "    B = ∇²f(x)\n",
    "    d_norm = norm(d)\n",
    "    \n",
    "    # Arrived at an optimal iterate x minimizing f?\n",
    "    if d_norm < ϵ\n",
    "        return p\n",
    "    end \n",
    "    \n",
    "    z = 0 # cg direction\n",
    "    r = -d\n",
    "    # solve tr subproblem by conjugate gradient\n",
    "    while true\n",
    "        if (α = d'*B*d) <= 0   # g is a direction of neg. curvature\n",
    "            return  d*Δ/d_norm\n",
    "        end\n",
    "        \n",
    "        temp = r'*r\n",
    "        z = z + temp/α * d\n",
    "        if (z_norm = norm(z)) >= Δ\n",
    "             return  z*Δ/z_norm\n",
    "        end\n",
    "\n",
    "        r = r + α*B*d\n",
    "        if (r_norm = norm(r)) <= ϵ\n",
    "            return z\n",
    "        end\n",
    "        d = -r + r_norm^2/temp * d\n",
    "    end\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quasi-Newton Methods\n",
    "\n",
    "These methods are classified by their unique ability to require only the gradient of the objective function at each iteration.\n",
    "By measuring the change in gradients between iterations, it can construct a model of the objective function, $f$, that produces superlinear convergence. We begin with the derivation of a Quasi-Newton approach by constructing a quadratic model of $f$ at an iterate $x_k$:  \n",
    "\n",
    "$$ m_k(p) = f_k + \\nabla f_K^T p + \\frac12 p^T B_K p, $$\n",
    "\n",
    "where $B_k$ is a square SPD matrix that will be updated at every iteration.\n",
    "\n",
    "pg. 146, safegaurd sr1, veryify with srmin  \n",
    "pg. 8 Fundamentals of Matrix Computations.... Block proofs needed to put algs in block form."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
