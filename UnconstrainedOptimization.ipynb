{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Convexity: Fundamental Property in Optimization\n",
        "\n",
        "Many practical problems in optimization are _convex_, which makes them easier to solve in theory and practice.  \n",
        "   \n",
        "- **def:** A set $S \\in \\mathbb{R}$ is a *convex set* if for any $x,y \\in S$, the following holds for all $\\alpha \\in [0,1]$ \n",
        "   $$\\alpha x + (1 - \\alpha)y \\in S.$$  \n",
        "\n",
        "- **def:** A function f is a *convex function* if its domain $S$ is a convex set and if for any $x,y \\in S$, the following holds for all $\\alpha \\in [0,1]$\n",
        "   $$f(\\alpha x + (1 - \\alpha) y) \\leq  \\alpha f(x) + (1-\\alpha) f(y).$$ If the above inequality holds strictly, we say $f$ is a *strictly convex function*.  \n",
        "\n",
        "- **def:** A function $f$ is said to *concave* if $-f$ is convex.  \n",
        "\n",
        "\n",
        "#### Examples of *convex sets*:  \n",
        "\n",
        "- The open ball about x or radius r, i.e. $B_r(x) = \\{ y \\in \\mathbb{R}^n : ||y - x||_2 \\leq r \\}$.  \n",
        "\n",
        "- Any polyhedron, i.e. the set defined by linear equalities and inequalities, $\\{ x \\in \\mathbb{R}^n : Ax = b, Cx \\leq d \\}$ where $A,C,b,d$ are appropriate dimensions.  \n",
        "\n",
        "#### Examples of *convex functions*:  \n",
        "\n",
        "- Any linear function of the form $f(x) = c^T x + \\alpha$ for any constant vector $c \\in \\mathbb{R}^n$ and $\\alpha \\in \\mathbb{R}$.  \n",
        "\n",
        "- The convex quadratic function $f(x) = x^T H x$, where $H$ is a symmetric positive semidefinite matrix (SPD).  \n",
        "\n",
        "If the objective function and feasible region, defined by the given constraints, are both convex, then any local solution of the problem is a global solution.  \n",
        "\n",
        "&nbsp; \n",
        "\n",
        "## Desired Iterative Optimization Algorithm Properties\n",
        "\n",
        "- **Robustness** - They perform on a wide variety of problems in their class, for any reasonable starting iterate.\n",
        "- **Efficiency** - In terms of time and space complexity\n",
        "- **Accuracy** - Identify a solution with precision, without being overly sensitive to errors in the data or floating-point arithmetic rounding errors.  \n",
        "\n",
        "&nbsp;  \n",
        "\n",
        "## Unconstrained Optimization\n",
        "\n",
        "The problem is to minimize an objective function that depends on real variables with no constratins, stated as $$\\min_{x \\in \\mathbb{R}^n} f(x)$$ where $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is a smooth function. \n",
        "\n",
        "#### Solution Types\n",
        "\n",
        "- **def:** A point $x^*$ is a *global minimizer* if $f(x^*) \\leq f(x) ~ \\forall x$.  \n",
        "\n",
        "- **def:** A point $x^*$ is a *local minimizer* if there is a neighboorhood $N$ of $x^*$ such that $f(x^*) \\leq f(x) ~ \\forall x \\in N$.  \n",
        "\n",
        "- **def:** A point $x^*$ is a *strict local minimizer* if there is a neighboorhood $N$ of $x^*$ such that $f(x^*) < f(x) ~ \\forall x \\in N$ with $x \\not = x^*$.  \n",
        "\n",
        "- **def:** A point $x^*$ is an *isolated local minimizer* if there is a neighboorhood $N$ of $x^*$ such that $x^*$ is the only local minimizer in $N$.  \n",
        "\n",
        "\n",
        "Ideally, our algorithm determines a global minimizer. \n",
        "\n",
        "&nbsp;\n",
        "\n",
        "## Solution Criterion  \n",
        "\n",
        "The mathematical tool to study minimizers of smooth functions is Taylor's Theorem, which allows us to derive _necessary_ and _sufficient_ conditions.  \n",
        "\n",
        "&nbsp;  \n",
        "\n",
        "**Theorem 2.1** (Taylor's Theorem): Suppose that $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is continuosly differentiable and $p \\in \\mathbb{R}^n$. Then we have that\n",
        "$$ f(x + p) = f(x) + \\nabla f(x + tp)^T p ,$$\n",
        "for some $t \\in (0,1)$. Moreover, if $f$ is twice continuously differentiable, we have have that\n",
        "$$ f(x + p) = f(x) + \\nabla f(x + tp)^T p  + \\frac{1}{2} p^T \\nabla^2 f(x + tp)p,$$\n",
        "for some $t \\in (0,1)$.  \n",
        "\n",
        "&nbsp; \n",
        "\n",
        "**Theorem 2.2** (First-Order Necessary Conditions): If $x^*$ is a local minimizer and $f$ is continuously differentiable in an open neighborhood of $x^*$, then $\\nabla f(x^*) = 0$.  \n",
        "\n",
        "&nbsp; \n",
        "\n",
        "**def:** We call $x^*$ a _stationary point_ if $\\nabla f(x^*) = 0$.  \n",
        "+ By theorem 2.2 any local minimizer must be a stationary point.  \n",
        "\n",
        "&nbsp; \n",
        "\n",
        "** Theorem 2.3** (Second-Order Necessary Conditions): If $x^*$ is a local minimizer of $f$ and $\\nabla^2 f$ exists and is continuous in an open neighborhood of $x^*$, then $\\nabla f(x^*) = 0$ and $\\nabla^2 f(x^*)$ is positive semidefinite.  \n",
        "\n",
        "&nbsp; \n",
        "\n",
        "** Theorem 2.4** (Second-Order Sufficient Conditions): Suppose that $\\nabla^2 f$ is continuous in an open neighborhood of $x^*$ and that $\\nabla f(x^*) =0$ and that $\\nabla^2 f(x^*)$ is positive definite. Then $x^*$ is a strict local minimizer of f.  \n",
        "\n",
        "&nbsp; \n",
        "\n",
        "** Theorem 2.5**: When $f$ is convex, any local minimizer $x^*$ is a global minimizer of $f$. If in addition $f$ is differentiable, then any stationary point $x^*$ is a global minimizer of $f$.  \n",
        "\n",
        "&nbsp; \n",
        "\n",
        "The results above provide the foundations for unconstrained optimization algorithms, which all seek a point where $\\nabla f(\\cdot) $ vanishes.  \n",
        "\n",
        "&nbsp; \n",
        "\n",
        "## Iterative Optimization Algorithms Strategies: Line Search and Trust Region\n",
        "\n",
        "\n",
        "Either of the two algorithm strategies seek begin with an initial guess $x_0$ and generate a sequence of iterates $(x_k)$ that terminates when either no more progress can be made or when it seems that a solution point has been approximated with sufficient accuracy.\n",
        "There are two approaches for deciding how to move from one iterate $x_k$ to the next, $x_{k+1}$:\n",
        "+ _montone_ algorithm's enforce that the next iterate satisfies $f(x_k) < f(x_{k+1})$  \n",
        "+ _nonmontone_ algorithms enforce that $f$ is decreased after a prescribed number of iterations $m$, i.e. $f(x_k) < f(x_{k-m})$ \n",
        "\n",
        "#### Line Search Strategy\n",
        "- Determine a search direction $p_k$ from the current itterate $x_k$.  \n",
        "- Pick a step-size $\\alpha$ that solves (or approximates) the one-dimensional minimization problem $\\arg \\min_{\\alpha > 0} f(x_k + \\alpha p_k)$.  \n",
        "- Update the incumbent solution to determine the next iterate $x_{k+1}$.  \n",
        "\n",
        "At $x_{k+1}$, the process repeats unless flaged otherwise, e.g. a lack of suffiecient progress or $x_{k+1}$ is a sufficient approximation.\n",
        "It must be noted, there is no guarantee of a rate of convergance that the algorithm will determine a sufficient approximation of the true solution $x^*$.  \n",
        "\n",
        "#### Trust Region Strategy\n",
        "- construct a model function $m_k$ who's behavior around the iterate $x_k$ is similiar to the behavior of $f$ around $x_k$.  \n",
        "    - determine a trust-region, where we believe $m_k$ is a good approximater of $f$.  \n",
        "- Solve the sub-problem $\\min_p m_k(x_k + p)$ where $x_k + p$ lies inside the trust region.  \n",
        "    - If the determined direction solution leads to a sufficient decrease, the new iterate is $x_{k+1} = x_k + p$ and we may allow for a larger trust-region on the next iteration.  \n",
        "    - Otherwise, the trust-region may have been to large and we constrain the search to a smaller region before progressing to the next iterate.  \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Line Search Algorithms  \n",
        "\n",
        "A line search algorithm must determine a search direction $p_k$ and a step length $\\alpha_k$.\n",
        "Most line search algorithms require $p_k$ to be a descent direction, i.e. $p_k^T \\nabla f_k < 0$, because this ensures that $f$ can be reduced along this direction.\n",
        "The search direction often has the form $$ p_k = -B_k^{-1} \\nabla f_k $$ where $B_k$ is a symmetric and nonsingular matrix.  \n",
        "- The steepest descent method sets $B_k$ to the indentity matrix, so you search in the direction given by the gradient.  \n",
        "- Newton's method set's $B_k$ to the exact Hessian $\\nabla^2 f(x_k)$.  \n",
        "- In quasi-Newton methods, $B_k$ is an approximation to the Hessian that is updated at every iteration by means of a low-rank formula.  \n",
        "\n",
        "When defining a search direction of such form, and enforcing the condition that $B_k$ is positive definite we have\n",
        "$$ p_k^T \\nabla f_k = (-B_k^{-1} \\nabla f_k)^{-1} \\nabla f_k = -\\nabla f_k^T B_k^{-1}\\nabla f_k < 0 $$\n",
        "and consequently, our choice of $p_k$ is a direction of descent. Then a step $\\alpha > 0$ leads to a monotone scheme.  \n",
        "\n",
        "\n",
        "At the $kth$ iteration, we must evaluate the tradeoff between solving $\\arg \\min_{\\alpha_k} f(x_k + \\alpha_k p_k)$ or following a lighter-cost procedure that determines an $\\alpha_k$ approximate of the one-dimensional minimization problem.\n",
        "Generally, a lighter-cost procedure is preffered and there exist many variants.\n",
        "However, determining such a procedure that produces convergence is not always an easy task, e.g. incruementing by a $\\Delta p$ until we have determined a $\\alpha_k = n \\Delta p$ such that $f(x_k + \\alpha_k p_k) < f(x_k)$ will not gaurentee convergence.\n",
        "There are many conditions discussed in chapter 3, each having there faults and utility based upon the method used in determining the search direction.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rosenbrock function \n",
        "function rosen(x)\n",
        "    fVal = 0.0\n",
        "    for i in 1:length(x)-1\n",
        "        fVal += 100.0*(x[i+1] - x[i]^2)^2 + (1.0-x[i])^2\n",
        "    end\n",
        "    return fVal\n",
        "end\n",
        "\n",
        "# it's derivative\n",
        "function rosend(x, dx)\n",
        "    fVal = df = 0.0; \n",
        "    for i in 1:length(x)-1\n",
        "       (fVal  += 100.0*(x[i+1] - x[i]^2)^2 + (1.0-x[i])^2;\n",
        "        df    += 200.0*(x[i+1] - x[i]^2)*(dx[i+1] -2.0*x[i]*dx[i] ) - 2.0*(1.0 - x[i])*dx[i])\n",
        "    end\n",
        "    return (fVal,df)\n",
        "end"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "rosend (generic function with 1 method)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "execution": {
          "iopub.status.busy": "2021-07-30T04:05:15.808Z",
          "iopub.execute_input": "2021-07-30T04:05:16.467Z",
          "iopub.status.idle": "2021-07-30T04:05:17.497Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**function line_search():**\n",
        "An iterative procedure for finding an approximation to the minimum of the one-dimensional function $f(x_k + \\alpha p_k)$.  \n",
        "The function returns an $\\alpha$ that satisfies the strong Wolfe termination conditions (section 3.1), using $c_1 = 10^{-4}$ and $c_2 = .9$.\n",
        "A step size produced by line_search(), with a given descent direction $p_k$, will ensure a convergent optimization procedure for objective\n",
        "functions that our continuously differentiable and Lipchnitz continuous. This should be used to determine a step size on general nonlinear functions $f$\n",
        "\n",
        "**Question**: Is the rosenbrock function applicable? It is a non-convex function, so we can't explicitly solve for a step $\\alpha$?\n",
        "I believe this to be correct, given any arbitrary descent direction $p_k$. \n",
        "\n",
        "However, we should be able to look at the eigenvalues of it's hessian.\n",
        "If the eigenvalues are all positive, then we know that the rosenbrock guy could be modeled by the quadratic of Taylor's theorem and we could use a trust-region method.\n",
        "Correct? \n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using ForwardDiff\n",
        "\n",
        "c1 = 10^(-4)\n",
        "c2 = 0.9\n",
        "\n",
        "# declare my forward diff \n",
        "f = x -> rosen(x)\n",
        "df = (x, p) ->p'*ForwardDiff.gradient(rosen, x)\n",
        "\n",
        "\n",
        "function zoom(x, p, a_l, a_h)\n",
        "    a_star = 0.0\n",
        "    while true\n",
        "        # what does interpolate using quadratic, cubic, or bisection mean?\n",
        "        a_m = (a_h - a_l)/2 + a_l\n",
        "        fx = f(x)\n",
        "        dfx = df(x, p)\n",
        "        fa_m = f(x + a_m .* p)\n",
        "\n",
        "\n",
        "        if fa_m > f(x) + c1*a_m*dfx || fa_m >= f(x + a_l .* p )\n",
        "            a_h = a_j\n",
        "        else \n",
        "            dfa_m = df(x + a_m .* p, p)\n",
        "            if abs(dfa_m) <= -c*dfx\n",
        "                a_star = a_m\n",
        "                break\n",
        "            end\n",
        "            if dfa_m * (a_h - a_l) >= 0\n",
        "                a_h = a_l\n",
        "            end\n",
        "            a_l = a_m\n",
        "        end\n",
        "    end\n",
        "    a_star\n",
        "end\n",
        "\n",
        "function line_search(f, df, x, p, a_max)\n",
        "    a_star = a_l = 0.0\n",
        "    a_h = a_max/2\n",
        "    fx = f(x)\n",
        "    dfx = df(x, p)\n",
        "\n",
        "    while true\n",
        "        fa_h = f(x + a_h.*p)\n",
        "        if (fa_h > fx + c1 * a_h .* dfx) || (f(x + a_h .* p) >= f(x + a_l .* p) && a_l != 0.0)\n",
        "            a_star = zoom(a_l, a_h)\n",
        "            break\n",
        "        end\n",
        "        dfa_h = df(x + a_h.*p, p)\n",
        "\n",
        "        if abs(dfa_h <= -c2 *dfx)\n",
        "            a_star = a_h \n",
        "            break\n",
        "        end\n",
        "        if df_a >= 0\n",
        "            a_star = zoom(a_h, a_l)\n",
        "            break\n",
        "        end\n",
        "        a_l = a_h\n",
        "        a_h = (a_max - a_h)/2 + a_h\n",
        "    end\n",
        "    a_star\n",
        "end\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "line_search (generic function with 1 method)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "execution": {
          "iopub.status.busy": "2021-07-30T04:05:17.508Z",
          "iopub.execute_input": "2021-07-30T04:05:17.514Z",
          "iopub.status.idle": "2021-07-30T04:05:19.219Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = rand(12)\n",
        "p = rand(12)\n",
        "\n",
        "println(\"Forward Diff gradient in direction of x: \", df(x, p))\n",
        "println(\"By Hand gradient in the direction of x: \", rosend(x, p))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward Diff gradient in direction of x: 282.92441921979866\n",
            "By Hand gradient in the direction of x: (221.39400690572955, 282.92441921979866)\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "execution": {
          "iopub.status.busy": "2021-07-30T04:24:07.163Z",
          "iopub.execute_input": "2021-07-30T04:24:07.171Z",
          "iopub.status.idle": "2021-07-30T04:24:07.189Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "julia-1.6"
    },
    "language_info": {
      "file_extension": ".jl",
      "name": "julia",
      "mimetype": "application/julia",
      "version": "1.6.2"
    },
    "kernelspec": {
      "argv": [
        "/Applications/Julia-1.6.app/Contents/Resources/julia/bin/julia",
        "-i",
        "--color=yes",
        "--project=@.",
        "/Users/daniel/.julia/packages/IJulia/e8kqU/src/kernel.jl",
        "{connection_file}"
      ],
      "display_name": "Julia 1.6.2",
      "env": {},
      "interrupt_mode": "signal",
      "language": "julia",
      "name": "julia-1.6"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}