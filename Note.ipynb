{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUTEst* and NLPModels.jl\n",
    "\n",
    "The CUTEst project is the modern and modular thread-safe predecessor of the CUTEr project, which was the most adopted Fortran package in the evolution of _Constrained and Unconstrained Testing Environment (CUTE)_ family for optimization problems. In addition to thread safety, CUTEst features dynamic memory allocation.\n",
    "The CUTEst additions were facilitated by a massive overhaul of the CUTEr's Fortran 77 codebase to Fortran 2003 release.\n",
    "Most of the changes where internal, and there is very little documentation for CUTEst. Instead, CUTEst generally refers you to the old CUTEr documentation.\n",
    " + Source - 9ish pages discussing the CUTEst updates: https://www.researchgate.net/publication/265164342_CUTEst_a_Constrained_and_Unconstrained_Testing_Environment_with_safe_threads_for_Mathematical_Optimization\n",
    " \n",
    "+ Source - the CUTEst GitHub repo, fairly wordy and useless. The docs directory is split up into a bunch of pdf's containing references to the Fortran CUTEr interface: https://github.com/ralna/CUTEst\n",
    "\n",
    "\n",
    "As for CUTEst.jl, it is a wrapper of the CUTEst project that extends the NLPModels.jl abstract Julia type, which attempts to standardize the interface for representing optimization problems in Julia.  \n",
    "The CUTEst.jl documentation is lacking; instead, it refers you to the CUTEst Fortran project (GitHub repo referenced above).\n",
    "The NLPModels documentation is quite good, so I am assuming CUTEst.jl implements everything needed to fulfill its subtype requirements.\n",
    "(See Basic requirements of the NLPModels docs:https://juliasmoothoptimizers.github.io/NLPModels.jl/dev/)\n",
    "\n",
    "\n",
    "***Question*** Where does AD fit into all this? After reading the AD wiki and scanning the paper cited in the ForwardDiff.jl package, it makes so much sense!\n",
    "What a great alternative and paradigm shift for computing differentials.\n",
    "The NLPModels.jl package says that programming problems must implement a grad() and a hess() method that accepts and NLPModels type, which CUTEST.jl does (as shown below). \n",
    "However, it is unclear if the CUTEST.jl package is using an Algorithmic/Automatic differentiation in their implementations.\n",
    "After scanning CUTEst.jl sourcecode, they make a lot of _ccall()'s_, which makes sense since it is a wrapper. I suppose they are calling a wrapper of the Fortran package, namely the C interface.\n",
    "So the question becomes, does the original CUTEst use AD?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Problem name: ROSENBR\n",
       "   All variables: ████████████████████ 2      All constraints: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            free: ████████████████████ 2                 free: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "          infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            nnzh: (  0.00% sparsity)   3               linear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                    nonlinear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                         nnzj: (------% sparsity)         \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 = [-1.2, 1.0]\n",
      "fx = 24.199999999999996\n",
      "gx = [-215.59999999999997, -87.99999999999999]\n",
      "Hx = [1330.0 480.0; 480.0 200.0]\n"
     ]
    }
   ],
   "source": [
    "using CUTEst, NLPModels\n",
    "\n",
    "# from the README.md of CUTEst julia package\n",
    "nlp = CUTEstModel(\"ROSENBR\")\n",
    "println(\"x0 = $(nlp.meta.x0)\")\n",
    "println(\"fx = $( obj(nlp, nlp.meta.x0) )\")\n",
    "println(\"gx = $( grad(nlp, nlp.meta.x0) )\")\n",
    "println(\"Hx = $( hess(nlp, nlp.meta.x0) )\")\n",
    "problems = CUTEst.select()\n",
    "display(nlp)\n",
    "typeof(nlp)\n",
    "finalize(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JuliaGPU\n",
    "\n",
    "As for computing on the GPU, it isn't going to work on my Mac. I have a Nvidia GeForce graphics card, which once supported CUDA. However, Apple and Nvidia got into it a while back, and Apple now has a bunch of system restrictions that no longer give you access to the Nvidia GPU on their most recent updates. (it is pretty comical from my reading) I tried to boot my machine in recover mode, override system privileges, and run an open-source CUDA driver executable, but it did not grant access to the GPU.\n",
    "\n",
    "It shouldn't be too much of an issue to tell Julia you have GPU on a Linux or Windows operating system (Assuming it is a nice ARM, Nvidia or Intel GPU). However, JuliaHub appears to be a great platform that allows for Cloud deployment and has all the backend taken care of, so it is as easy as putting _Pkg.add(JuliaGPU), using JuliaGPU_ in your notebook. It costs about $0.32/per hour at a minimum and scales up rapidly when you try to distribute the load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
