{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CUTE Classification Scheme\n",
    "\n",
    "Each _CUTE_ problem is assigned a string identifire in it's SIF encoding, which has the form:\n",
    "\n",
    "`**XXXr-XX-n-m**`  \n",
    "\n",
    "The **X** characters do not need to be present in the origonal FORTRAN tools that queries SIF CLASS.DB file, see \n",
    "https://www.cuter.rl.ac.uk/Problems/classification.shtml for more information.\n",
    "\n",
    "## CUTEst.jl\n",
    "\n",
    "It appears the CUTEst.jl package contains problems classified under the scope of Test, which is encoded SIF classification in the first **X** to the right of the first hyphen. Such test set problems are listed here https://www.cuter.rl.ac.uk/Problems/mastsif.html. Furthermore, every problem in the test set has a first charachter of '2' left of first hypen; suggesting that we test our algorithms on problems that have an analytical computation for the Hessian. (Note, CUTEst.jl belongs to the JuliaSmoothOptimizers orginization)\n",
    "\n",
    "In CUTEst.jl (v0.12) they have a tool called `CUTEst.select(...)` that scans the set of Test Problems and queiries a subset corresponding to the given arguments.\n",
    "For more information `ctrl+F` _Selection tool_ here http://juliasmoothoptimizers.github.io/CUTEst.jl/v0.12/tutorial/#Selection-tool\n",
    "\n",
    "\n",
    "\n",
    "## FORTRAN Tool\n",
    "There does exist a tool in the SIFDecode artifact directory that is created when adding CUTEst.jl, called `slct.f`. The command line tool `slct.f` should work when your enviroment variables are exported into your shells path, as explained in https://github.com/ralna/CUTEst. When mine are not exported to my _~/.zshrc_, a segmentation fault occurs. You can find the `slct.f` tool in the path relative to your Julia installation directory, i.e. .julia/artifacts/{long shasum hash}/libexec/SIFDecode-2.0.3/src/select  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUTEst, NLPModels\n",
    "\n",
    "# selecting unconstrained problems:\n",
    "problems = CUTEst.select(contype=\"unc\")\n",
    "length(problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JuliaSmoothOptimizers (JSO)\n",
    "\n",
    "The orginization behind CUTEst.jl, NLPModels.jl, ADNLPModels.jl (an abstract framework for AD in NLP models developed with ForwardDiff.jl in mind) is JuliaSmoothOptimizers.\n",
    "\n",
    "#### Code disscusion\n",
    "The **newton_cg** function below is a JSO complient solver that constructs a trust-region sub-problem using Krlov.jl. Krylov.jl performs a conjugate gradient method to solve the subproblem. \n",
    "\n",
    "**Refrence:**  \n",
    "https://juliasmoothoptimizers.github.io/pages/tutorials/creating-a-jso-compliant-solver/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.999999935027122, 0.9999998679018123], 4.684772947787624e-15, 8.483643270898984e-7)"
     ]
    }
   ],
   "source": [
    "using Krylov, LinearAlgebra\n",
    "\n",
    "function newton_cg(nlp :: AbstractNLPModel)\n",
    "  x = nlp.meta.x0\n",
    "  fx = obj(nlp, x)\n",
    "  gx = grad(nlp, x)\n",
    "  ngx = norm(gx)\n",
    "  while norm(gx) > 1e-6\n",
    "    Hx = hess_op(nlp, x)\n",
    "    d, _ = cg(Hx, -gx)\n",
    "    slope = dot(gx, d)\n",
    "    if slope >= 0 # Not a descent direction\n",
    "      d = -gx\n",
    "      slope = -dot(d,d)\n",
    "    end\n",
    "    t = 1.0\n",
    "    xt = x + t * d\n",
    "    ft = obj(nlp, xt)\n",
    "    while ft > fx + 0.5 * t * slope\n",
    "      t *= 0.5\n",
    "      xt = x + t * d\n",
    "      ft = obj(nlp, xt)\n",
    "    end\n",
    "    x = xt\n",
    "    fx = ft\n",
    "    gx = grad(nlp, x)\n",
    "    ngx = norm(gx)\n",
    "  end\n",
    "  return x, fx, ngx\n",
    "end\n",
    "\n",
    "# test it on 2D-Rosenbrock function\n",
    "nlp = CUTEstModel(\"ROSENBR\")\n",
    "print(newton_cg(nlp))\n",
    "finalize(nlp) # you must always finalize the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearOperators.jl\n",
    "\n",
    "This package is the cornerstone of efficient design of nonlinear optimization algorithms through the JuliaSmoothOptimizers package.\n",
    "We perform an exploration of the package below, which is compatible with Julia 1.3 and up. \n",
    "\n",
    "`LinearOperator():` defines a linear transformation\n",
    " - v -> Av. \n",
    " - v -> A'v  \n",
    " - v -> A*v\n",
    " \n",
    "There are many advantages of using LinearOperators instead of working with matrices\n",
    "\n",
    "**Reference:**\n",
    "https://juliasmoothoptimizers.github.io/LinearOperators.jl/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear operator\n",
       "  nrow: 2\n",
       "  ncol: 2\n",
       "  eltype: Float64\n",
       "  symmetric: false\n",
       "  hermitian: false\n",
       "  nprod:   0\n",
       "  ntprod:  0\n",
       "  nctprod: 0\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearOperators\n",
    "\n",
    "prod(v) = [v[1] + v[2]; 2v[1] + 3v[3]]\n",
    "tprod(v) = [v[1] + 2v[2]; v[1] + 3v[2]]\n",
    "A = LinearOperator(Float64, 2, 2, false, false, prod, tprod, tprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.712129 seconds (2.40 M allocations: 129.915 MiB, 6.33% gc time, 99.43% compilation time)\n"
     ]
    }
   ],
   "source": [
    "A = rand(500, 500)\n",
    "B = rand(500, 500)\n",
    "@time A*B;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.026699 seconds (36.23 k allocations: 2.230 MiB, 99.01% compilation time)\n"
     ]
    }
   ],
   "source": [
    "opA = LinearOperator(A)\n",
    "opB = LinearOperator(B)\n",
    "@time opA*opB;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.073580 seconds (199.73 k allocations: 13.926 MiB, 92.09% compilation time)\n",
      "  0.000316 seconds (2 allocations: 8.125 KiB)\n",
      "  0.080612 seconds (211.75 k allocations: 12.768 MiB, 20.78% gc time, 99.28% compilation time)\n",
      "  0.008182 seconds (6.72 k allocations: 420.572 KiB, 93.09% compilation time)\n"
     ]
    }
   ],
   "source": [
    "v = rand(500)\n",
    "\n",
    "@time (A * B) * v\n",
    "@time A * (B*v)\n",
    "@time (opA * opB) * v\n",
    "@time opA * (opB * v);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note a linear operator is nearly a wrapper of a matrix, but there are some differences (e.g. slicing)\n",
    "A * ones(500) == opA * ones(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
