{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Conjugate Gradient Method (CGM)\n",
    "\n",
    "The linear conjugate gradient method is used for solving a linear system of equations $$Ax = b$$\n",
    "where $A$ is an $n\\times n$ symmetric positive definite matrix.  \n",
    "\n",
    "Such an $x$ can be determined by solving the equivalent convex minimization problem $$ \\min_{x\\in\\mathbb{R}^n} \\phi(x) = \\frac{1}{2} x^T A x - b^T x.$$  \n",
    "\n",
    "\n",
    "Thus, we can interpret the iterative Linear Conjugate Gradient algorithm as a technique for solving linear systems or as a means for minimizing convex quadratic functions.  \n",
    "\n",
    "Note, that $\\nabla \\phi$ is the residual of the linear system, i.e. $$\\nabla \\phi (x) = Ax - b = r(x),$$\n",
    "\n",
    "and for a given $x_k$ we say $r_k = r(x_k)$.\n",
    "\n",
    "The CGM generates a _conjugacy_ set of nonzero vectors $\\{p_0, p_1,...,p_l\\}$ in an economical fashion, a very remarkable property of the method.\n",
    "Having a set of vectors conjugate to $A$ will allow us to minimize $\\phi$ in $n$ steps by successively minimizing it along the individual directions in the conjugate set.  \n",
    "\n",
    "***def:*** A set of nonzeros vectors $\\{p_0, p_1,...,p_l\\}$ is said to be _conjugate_ to a SPD matrix $A$ if $p^T_iAp_j = 0 ~ \\forall i\\not = j$.  \n",
    "\n",
    "\n",
    "Let $x_o \\in \\mathbb{R}^n$ be a given starting point, and let $\\{p_0, p_1,...,p_l\\}$ be a set of conjugate directions with respect to $A$.\n",
    "Then we define the sequence $(x_k)$ by \n",
    "\n",
    "$$x_{k+1} = x_k + \\alpha_k p_k,$$  \n",
    "where $\\alpha_k$ is one-dimensional minimizer of $\\phi$ given by  \n",
    "$$\\alpha_k = -\\frac{r_k^T p_k}{p_k^TAp_k}.$$  \n",
    "\n",
    "&nbsp;  \n",
    "***Theorem 5.1:*** For any $x_0 \\in \\mathbb{R}^n$ the sequence $(x_k)$ converges to the global minimizer solution $x^*$ in at most n steps.  \n",
    "+ The recursively generated sequence $(x_k)$ is reffered to as the **conjugate direction algorithm**.  \n",
    "\n",
    "&nbsp;  \n",
    "***Theorem 5.2:*** For any starting point $x_0 \\in \\mathbb{R}^n$ and suppose $(x_k)$ is generated using the conjugate direction algorithm. Then  \n",
    "\n",
    "$$r_k^T p_i = 0 ~ \\forall i \\in [0, k-1],$$  \n",
    "\n",
    "and $x_k$ is the minimizer of $\\phi(x) = \\frac{1}{2} x^T A x - b^T x$ over the set $\\{x ~|~ x = x_o + \\text{span}\\{p_0, p_1,..., p_{k-1}\\} \\}$  \n",
    "+ This asserts that the current residual $r_k$ is orthogonal to all previous search directions, a crucial property to the algorithms utility.\n",
    "\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "If the Hessian of $\\phi$ happens to be a diagonal (note, $\\nabla^2 \\phi = A$), then the set of basis vectors will suffice for a conjugate direction set.\n",
    "When this property doesn't hold, the eigenvectors are both conjugate and orthogonal to the span of $A$ but this is generally an expensive computation.\n",
    "We could drop the orthogonality requirement and modify the Gram-Schmidt algorithm to produce a conjugate set, but that has trade-off's in terms of space complexity.  \n",
    "\n",
    "\n",
    "A beter approach would be to use the **CGM**, which is both a conjugate direction method and a procedure for generating a conjugate set.\n",
    "The CGM only requires knowledge of the previous conjugate vector, namely, given p_{k-1} it can produce a new vector $p_k$ such that $p_{j}^T A p_k = 0$ for $j < k$.\n",
    "This is done, by picking the next conjugate vector by a linear combination of the residual $r_k$ (which is $\\nabla \\phi(x_k)$ and the previous conjugate direction, i.e.  \n",
    "\n",
    "$$p_k = - r_k + \\beta_k p_{k-1},$$\n",
    "\n",
    "where the scalar $\\beta_k$ is to be determined by the requirement that $p_{k-1}$ and $p_k$ must be conjugate with respect to A.\n",
    "By premultiplying $p_{k-1}^T A$ to both sides of the previous equation we can solve for $\\beta_k$ as  \n",
    "\n",
    "$$\\beta_k = \\frac{r_k^T A p_{k-1}}{p_{k-1}^T A p_{k-1}}.$$\n",
    "\n",
    "Lastly, we can safely choose the initial direction $p_o$ by the fact that $\\phi$ is convex, namely, $p_0 = \\nabla \\phi(x_0)$ where $x_0$ is the initial guess.\n",
    "\n",
    "We now can specify a preliminary conjugate gradient method! But always remeber that it is not the gradients that are conjugate, rather, the search directions that are conjugate!\n",
    "\n",
    "**function cgm_linear($A$, $b$, $x$, $\\epsilon$):** Parameters are $A \\in \\mathbb{R}^{n \\times n}$ SPD, $x, b \\in \\mathbb{R^n}$, and $\\epsilon$ is a specified precision. The algorithm terminates when it has identified an global minimum or more than $n$ iterations have occured. It returns $x^*$, the determined global minimizer. \n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cgm_linear (generic function with 1 method)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function cgm_linear(A, b, x, eps) \n",
    "    # initialize variables to the kth iteration\n",
    "    r = A*x-b\n",
    "    p = -r\n",
    "    k  = 0\n",
    "    while norm(r) > eps\n",
    "        x -= ((r'*p) / (p'*A*p)) * p\n",
    "        r = A*x - b\n",
    "        p = -r + ((r'*A*p) / (p'*A*p)) * p  \n",
    "        if (k+=1) > length(x) # theroem 2.1 asserts the LHS should be near length(x), i.e. n?\n",
    "            println(\"cgm_linear: terminating before meeting specified precision\")\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    println(\"Number of iterations k = \", k)\n",
    "    x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations k = 11\n",
      "Euclidean error: 0.004677132925527356\n"
     ]
    }
   ],
   "source": [
    "# perform some basic testing of cgm_linear\n",
    "\n",
    "n = 20\n",
    "A1 = Diagonal(rand(n))\n",
    "b1 = rand(n)\n",
    "x1 = ones(n)\n",
    "\n",
    "x_star = cgm_linear(A1, b1, x1, 10^(-3))\n",
    "println(\"Euclidean error: \", norm(A1\\b1 - x_star))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cgm_linear: terminating before meeting specified precision\n",
      "Number of iterations k = 21\n",
      "Euclidean error: 7.527414390706144e-8\n"
     ]
    }
   ],
   "source": [
    "# perform another test with a dense matrix.. this one regularly terminates early on large system?\n",
    "# but we are close, for small n around 20!\n",
    "\n",
    "temp = rand(n,n)\n",
    "A2 = (temp + temp')\n",
    "b2 = rand(n)\n",
    "x2 = ones(n)\n",
    "\n",
    "x_star = cgm_linear(A2, b2, x2, 10^(-3))\n",
    "println(\"Euclidean error: \", norm(A2\\b2 - x_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fact that the residuals at each iteration are mutually orthogonal, and since each search direction $p_k$ and residual $r_k$ is contained in the _Krylov subspace of degree k for $r_0$_ ***defined*** as  \n",
    "\n",
    "$$ \\kappa (r_0; k) \\equiv \\text{span } \\{r_0, Ar_0,...., A^k r_0\\}, $$\n",
    "\n",
    "we can devise a more efficient linear CGM variant by the next theorem.\n",
    "\n",
    "&nbsp;  \n",
    "***Theorem 5.3:***  Suppose that the $k$th iterate generated by the conjugate gradient method is not the solution point $x^*$. The following properties hold:  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "r_k^Tr_i &= 0, \\text{ for } i = 0, 1, ..., k-1, \\\\\n",
    "\\text{span } \\{r_0, r_1,..., r_l \\} & = \\text{span }\\{r_0, Ar_0,...., A^k r_0\\}, \\\\\n",
    "\\text{span } \\{p_0, p_1,..., p_l \\} & = \\text{span }\\{r_0, Ar_0,...., A^k r_0\\}, \\\\\n",
    "p^t_k A p_i & = 0, \\text{ for } i = 0, 1, ..., k-11. \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, the sequence $(x_k)$ converges to $x^*$ in at most n steps.  \n",
    "\n",
    "The proof is by induction, relying on the fact the the first direction $p_0$ is the steepest descent directions $-r_0 = -\\nabla \\phi$. Furthermore, it is a necessitiy that our $p_0$ is chosen this way, otherwise the result doesn't hold.  \n",
    "&nbsp;\n",
    "Using our new results, we can use a new step:\n",
    "\n",
    "$$ \\alpha_k = \\frac{r^T_kr_k}{p^T_k A p_k}. $$\n",
    "\n",
    "We can also simply our formula for $\\beta_{k+1}$:\n",
    "\n",
    "$$ \\beta_{k+1} = \\frac{r^T_{k+1} r_{k+1}}{r^T_k r_k}. $$\n",
    "\n",
    "\n",
    "We make the update, and devise the ***function cgm_lin_fast***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cgm_linear_fast (generic function with 1 method)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cgm_linear_fast(A, b, x, eps) \n",
    "    # initialize variables to the kth iteration\n",
    "    r = A*x-b\n",
    "    p = -r\n",
    "    k  = 0\n",
    "    while norm(r) > eps\n",
    "        a = (r'*r) / (p'*A*p)\n",
    "        x +=  a* p\n",
    "        r_old = r\n",
    "        r = r + a*A*p\n",
    "        p = -r + ((r'*r) / (r_old'r_old)) * p  \n",
    "        if (k+=1) > length(x) # theroem 2.1 asserts the LHS should be near length(x), i.e. n?\n",
    "            println(\"cgm_linear: terminating before meeting specified precision\")\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    println(\"Number of iterations k = \", k)\n",
    "    x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations k = 11\n",
      "Euclidean error: 0.004677132925527564\n",
      "\n",
      "cgm_linear: terminating before meeting specified precision\n",
      "Number of iterations k = 21\n",
      "Euclidean error: 3.0236157920638187e-6\n"
     ]
    }
   ],
   "source": [
    "x_star = cgm_linear_fast(A1, b1, x1, 10^(-3))\n",
    "println(\"Euclidean error: \", norm(A1\\b1 - x_star), \"\\n\")\n",
    "\n",
    "\n",
    "x_star = cgm_linear_fast(A2, b2, x2, 10^(-3))\n",
    "println(\"Euclidean error: \", norm(A2\\b2 - x_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results appear to be quite identical between the two variants!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
